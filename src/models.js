// src/models.js

const models = [
  {
    id: "RBM",
    model: "Restricted Boltzmann Machine",
    paperTitle: "Information Processing in Dynamical Systems: Foundations of Harmony Theory (Harmonium, precursor of RBM)",
    venue: "Parallel Distributed Processing: Explorations in the Microstructure of Cognition (book chapter)",
    year: 1986,
    exactDate: "1986-01-02",
    citations: 3101,
    pdf: "https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap6_PDP86.pdf",
    openSource: "https://github.com/echen/restricted-boltzmann-machines",
    type: "EBM",
    color: "#FFA500",
    primaryUseCase: "Image",
    bestPerformance: "Qualitative circuit problem: 93% correct completion rate",
    flowchart: "r, a → H(r, a) → e^{H/T} → sample states → maximize H",
    highLevelIntuition: "System represents input as feature and activation vectors.\nIt computes a harmony score, converts this to probabilities, samples states, and iterates to maximize harmony.",
    pros: [
      "Dynamically creates context-sensitive representations",
      "Can model both symbolic and subsymbolic processing",
      "Captures learning via unsupervised exposure"
    ],
    cons: [
      "Requires careful tuning of parameters (e.g., computational temperature)",
      "Scalability limited by two-layer restriction vs. general Boltzmann machines",
      "Interpretability of internal representations can decrease with complexity"
    ],
    analogy: "Imagine a chef tossing a bowl of mixed raw veggies and spices, tasting random samples after each toss, and constantly re-mixing the bowl to find the most harmonious blend, repeating this process until every spoonful feels perfectly balanced and satisfying.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PoE",
    model: "Product of Experts",
    paperTitle: "Products of Experts",
    venue: "ICANN",
    year: 1999,
    exactDate: "1999-09-10",
    citations: 583,
    pdf: "https://www.cs.toronto.edu/~hinton/absps/icann-99.pdf",
    openSource: "https://github.com/hyoseok1223/Product-of-Experts-GAN",
    type: "EBM",
    color: "#FFA500",
    primaryUseCase: "Image",
    bestPerformance: "Accurate modeling of high-dimensional data with intersecting constraints (e.g., USPS digit images: almost perfect reconstruction and 99%+ classification accuracy)",
    flowchart: "d → [each expert scores: p₁(d), ..., pₙ(d)] → [multiply] → p(d) → [Gibbs sample] → c",
    highLevelIntuition: "Each expert checks its preferred constraint.\nMultiply all expert scores for sharp joint fit.\nSample fantasy data to train.",
    pros: [
      "Sharp, efficient modeling of high-dimensional data",
      "Each expert specializes in a low-dimensional constraint",
      "Tractable inference when experts are simple"
    ],
    cons: [
      "Requires iterative sampling (Gibbs) for generation",
      "Difficult partition function/normalization",
      "Generation is slower than mixture models"
    ],
    analogy: "Picture several chefs, each with their own strict rule for seasoning a soup—one insists it must be spicy, another demands it’s always sour, a third checks for perfect salt—so every spoonful is only served if it passes all their simultaneous taste tests, ensuring only the most precisely balanced flavors make it to the table.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DBN",
    model: "Deep Belief Network",
    paperTitle: "A fast learning algorithm for deep belief nets",
    venue: "Neural Computation",
    year: 2006,
    exactDate: "2006-07-01",
    citations: 22965,
    pdf: "https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf",
    openSource: "https://github.com/mehulrastogi/Deep-Belief-Network-pytorch",
    type: "EBM",
    color: "#FFA500",
    primaryUseCase: "Image",
    bestPerformance: "MNIST Test Error: 1.25%",
    flowchart: "flowcharts/gifs/DBN.gif",
    highLevelIntuition: "Each RBM layer learns features from the previous layer.\nThe top undirected layer connects features and labels to model data distribution.",
    pros: [
      "Fast, greedy layer-wise unsupervised learning",
      "Outperforms best discriminative models on MNIST",
      "Easily interpretable distributed representations"
    ],
    cons: [
      "Limited handling of non-binary natural images",
      "Lacks mechanisms for perceptual invariance/segmentation",
      "Fine-tuning is computationally slow"
    ],
    analogy: "Imagine stacking layers of flavor by first marinating vegetables, then grilling them, then simmering them in a stew, where each stage draws out new notes from the last, and finally tasting the whole pot to decide the final dish’s identity.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DBM",
    model: "Deep Boltzmann Machine",
    paperTitle: "Deep Boltzmann Machines",
    venue: "AISTATS",
    year: 2009,
    exactDate: "2009-04-16",
    citations: 3230,
    pdf: "https://www.cs.toronto.edu/~fritz/absps/dbm.pdf",
    openSource: "https://github.com/tjake/rbm-dbn-mnist",
    type: "EBM",
    color: "#FFA500",
    primaryUseCase: "Image",
    bestPerformance: "MNIST test error: 0.95%",
    flowchart: "flowcharts/gifs/DBM.gif",
    highLevelIntuition: "Input passes up through layers to extract features.\nEach layer refines its representation via feedback and sampling.",
    pros: [
      "Learns deep, hierarchical representations",
      "Handles millions of parameters robustly",
      "Strong unsupervised and semi-supervised performance"
    ],
    cons: [
      "Slow training without pretraining",
      "Inference and learning are approximate",
      "Computationally intensive for large datasets"
    ],
    analogy: "Imagine building a layered casserole, where each layer of vegetables is seasoned and partially cooked, then you keep passing the whole dish back and forth between oven and tasting table, letting flavors rise, settle, and interact until every bite reflects the combined depth of all layers.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "RNADE",
    model: "Real valued NADE",
    paperTitle: "RNADE: The real-valued neural autoregressive density-estimator",
    venue: "NeurIPS",
    year: 2013,
    exactDate: "2013-06-02",
    citations: 291,
    pdf: "https://arxiv.org/pdf/1306.0186",
    openSource: "https://github.com/MarcCote/NADE",
    type: "AR",
    color: "#FF6B00",
    primaryUseCase: "Audio",
    bestPerformance: "TIMIT speech test log-likelihood: 124.5 nats (RNADE-MoG K=20)",
    flowchart: "x₁ → [hidden h₁] → [p(x₁)]\nx₂ → [h₂(x₁)] → [p(x₂ | x₁)]\n⋮\nx_D → [h_D(x_{<D})] → [p(x_D | x_{<D})]",
    highLevelIntuition: "At each step, use previous values to compute a neural hidden state.\nThe hidden state predicts a Gaussian mixture for the next value.",
    pros: [
      "Tractable and exact likelihood computation",
      "Outperforms mixture models on most datasets",
      "Learns flexible, distributed representations"
    ],
    cons: [
      "Multiple hyperparameters require tuning",
      "Can produce out-of-range samples without output constraints",
      "Model performance sensitive to training procedure"
    ],
    analogy: "A chef slices and tastes each vegetable in a set order, letting the flavor and texture of every piece already added guide how they chop, season, and blend the next, ensuring each ingredient’s prep depends on all that came before.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1306.0186",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "GSN",
    model: "Generative Stochastic Network",
    paperTitle: "Generative Stochastic Networks",
    venue: "ICML",
    year: 2014,
    exactDate: "2013-06-05",
    citations: 78,
    pdf: "https://arxiv.org/pdf/1503.05571",
    openSource: "https://github.com/yaoli/GSN",
    type: "EBM",
    color: "#FFA500",
    primaryUseCase: "Image",
    bestPerformance: "“Deep GSN mixes faster than DBM; sharper MNIST samples”",
    flowchart: "X₀ → [noise/corrupt] → H₁ → [decode] → X₁ → [noise/corrupt] → H₂ → [decode] → X₂ → …",
    highLevelIntuition: "Inject noise into X to get H. Decode H to reconstruct X. Alternate these to form a Markov chain whose stationary distribution matches the data.",
    pros: [
      "Generalizes denoising autoencoders (DAEs)",
      "Avoids intractable partition functions",
      "Enables efficient training via backpropagation"
    ],
    cons: [
      "Requires sufficient model capacity",
      "Quality depends on transition operator design",
      "May need tuning for complex/multimodal data"
    ],
    analogy: "A chef deliberately messes up a dish by spilling or mixing in odd spices, then repeatedly fixes and tastes it, each time using what’s left to restore the recipe, until the final result tastes like the original meal despite all the chaos.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1503.05571",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "VAE",
    model: "Variational Auto Encoder",
    paperTitle: "Auto-Encoding Variational Bayes",
    venue: "ICLR",
    year: 2013,
    exactDate: "2013-12-20",
    citations: 47136,
    pdf: "https://arxiv.org/pdf/1312.6114.pdf",
    openSource: "https://github.com/pytorch/examples/tree/main/vae",
    type: "VAE",
    color: "#FFD700",
    primaryUseCase: "Image",
    bestPerformance: "MNIST marginal log-likelihood (3 latent): −100 nats",
    flowchart: "x → [Encoder NN] → (μ, σ) → [sample ε] → z = μ + σ ⊙ ε → [Decoder NN] → x̂",
    highLevelIntuition: "Encode x into latent code parameters (μ, σ).\nSample z from this distribution using reparameterization.\nDecode z to reconstruct x̂.\nOptimize so reconstructions are accurate and z’s distribution stays close to prior.",
    pros: [
      "Scalable to large datasets via minibatch SGD",
      "Efficient approximate inference with continuous latent variables",
      "Regularizes automatically via variational lower bound"
    ],
    cons: [
      "Posterior approximation limited by chosen variational family",
      "Potentially blurry generations for complex data",
      "Intractable for exact likelihoods in high dimensions"
    ],
    analogy: "A chef tastes a soup and writes a flexible recipe describing the most likely flavors and textures, then randomly selects ingredients using that recipe, cooks a new soup, and keeps revising both the recipe and results so that each remake is close to the original and follows the same general style.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1312.6114",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "NICE",
    model: "Nonlinear Indep. Components Est.",
    paperTitle: "NICE: Non-linear Independent Components Estimation",
    venue: "ICLR",
    year: 2015,
    exactDate: "2014-10-30",
    citations: 2923,
    pdf: "https://arxiv.org/pdf/1410.8516",
    openSource: "https://github.com/DakshIdnani/pytorch-nice",
    type: "Flow",
    color: "#00BFFF",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 log-likelihood: 5371.78",
    flowchart: "x₀ → [f] → h → [sample] → h̃ → [f⁻¹] → x̃",
    highLevelIntuition: "Transform data into latent space where components are independent.\nSample in latent space, invert transform to get new data.",
    pros: [
      "Exact log-likelihood, tractable for training",
      "Efficient unbiased ancestral sampling",
      "Simple, invertible architecture with tractable Jacobian"
    ],
    cons: [
      "Volume-preserving layers need extra scaling",
      "Restricted transformation expressiveness vs. fully general bijections",
      "Lower sample quality than VAEs/GANs on natural images"
    ],
    analogy: "A chef takes a mixed vegetable stew and skillfully sorts and separates every ingredient with a series of clever moves, then randomly swaps in new pieces in this separated form, finally reversing each step in perfect order to blend everything back into a fresh stew.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1410.8516",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "CGAN",
    model: "Conditional GAN",
    paperTitle: "Conditional Generative Adversarial Nets",
    venue: "arXiv",
    year: 2014,
    exactDate: "2014-11-06",
    citations: 15817,
    pdf: "https://arxiv.org/pdf/1411.1784",
    openSource: "https://github.com/eriklindernoren/PyTorch-GAN",
    type: "GAN",
    color: "#32CD32",
    primaryUseCase: "Image",
    bestPerformance: "MNIST Parzen log-likelihood: 132 ± 1.8",
    flowchart: "flowcharts/gifs/CGAN.gif",
    highLevelIntuition: "Noise and label are combined to generate a conditional sample.\nDiscriminator checks if sample matches label and is real or fake.",
    pros: [
      "Enables class-conditional sample generation",
      "Flexible conditioning on labels or other modalities",
      "Simple extension of standard GANs"
    ],
    cons: [
      "Proof-of-concept, not state-of-the-art",
      "May underperform non-conditional GANs",
      "Requires careful architecture/hyperparameter tuning"
    ],
    analogy: "A chef receives both surprise ingredients and a recipe card (like “make spicy curry”), invents a dish that fits the instructions, and a judge then tastes each creation, checking if it truly matches the recipe and wasn’t faked with the wrong flavors.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1411.1784",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "MADE",
    model: "Masked Autoencoder for Distribution Estimation",
    paperTitle: "MADE: Masked Autoencoder for Distribution Estimation",
    venue: "NeurIPS",
    year: 2015,
    exactDate: "2015-02-12",
    citations: 1140,
    pdf: "https://arxiv.org/pdf/1502.03509",
    openSource: "https://github.com/mgermain/MADE",
    type: "AR",
    color: "#FF6B00",
    primaryUseCase: "Tabular",
    bestPerformance: "Binarized MNIST NLL: 86.64",
    flowchart: "x₀ → [masking] → h → [masking] → x̂ → [product] → p(x)",
    highLevelIntuition: "Masked connections ensure each output depends only on prior variables.\n\nHidden layers transform input with constraints.\n\nOutputs give conditional probabilities, whose product is the full joint.",
    pros: [
      "Fast: Single forward pass computes full joint probability",
      "Flexible: Supports deep networks and multiple variable orderings",
      "Scalable: Efficiently vectorized, GPU-friendly"
    ],
    cons: [
      "Limited to discrete data (primarily binary)",
      "Over-regularization risk with too many masks",
      "Slightly less accurate than complex ensembles on some datasets"
    ],
    analogy: "A chef prepares a salad by adding each vegetable in a strict sequence, using clever dividers so each new ingredient’s seasoning depends only on what’s already in the bowl, then multiplies the taste results of every step to decide the overall flavor.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1502.03509",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DRAW",
    model: "DRAW",
    paperTitle: "DRAW: A Recurrent Neural Network For Image Generation",
    venue: "ICML",
    year: 2015,
    exactDate: "2015-02-16",
    citations: 2632,
    pdf: "https://arxiv.org/pdf/1502.04623",
    openSource: "https://github.com/ericjang/draw",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Image",
    bestPerformance: "Binarized MNIST: 80.97 nats (test negative log-likelihood)",
    flowchart: "flowcharts/gifs/DRAW.gif",
    highLevelIntuition: "At each step, the model reads a patch, encodes and samples latent info, then writes to the canvas.\nRepeats this process, gradually refining the image.",
    pros: [
      "Iterative, human-like image generation",
      "Differentiable spatial attention mechanism",
      "Substantially improves state of the art on MNIST"
    ],
    cons: [
      "Underfits complex datasets (e.g., SVHN)",
      "Blurry samples on natural images (e.g., CIFAR-10)",
      "Requires fixed number of time steps"
    ],
    analogy: "A chef sketches a painting of a dish by glancing at a reference, adding small strokes and colors bit by bit, tasting and adjusting after each addition, gradually layering details until the full meal appears vivid and complete.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1502.04623",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "FVSBN",
    model: "Fully Visible Sigmoid Belief Network",
    paperTitle: "Learning Deep Sigmoid Belief Networks with Data Augmentation",
    venue: "AISTATS",
    year: 2015,
    exactDate: "2015-02-21",
    citations: 132,
    pdf: "https://proceedings.mlr.press/v38/gan15.pdf",
    openSource: "https://github.com/linzangsc/reproduction-of-FVSBN",
    type: "EBM",
    color: "#f97316",
    primaryUseCase: "Image",
    bestPerformance: "Test log-prob. (MNIST, Gibbs, 200 h): −94.30",
    flowchart: "h^{(L)} → [sigmoid, W^{(L)}] → h^{(L-1)} → ... → h^{(1)} → [sigmoid, W^{(1)}] → v",
    highLevelIntuition: "Start with random binary codes at the top.\nEach layer transforms the codes downward via sigmoid and weights until generating the data.",
    pros: [
      "Fully Bayesian learning of deep directed models",
      "Sparsity-encouraging priors reduce overfitting",
      "Efficient Gibbs sampling and variational inference"
    ],
    cons: [
      "Computationally expensive for large datasets",
      "Only demonstrated on binary data",
      "Posterior inference is more complex than for undirected models"
    ],
    analogy: "A chef begins with a secret combination of rare spice powders at the top shelf, sprinkling and blending them layer by layer through sieves, with each stage mellowing and transforming the mix, until a finished, complex flavor emerges at the table.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PFlows",
    model: "Planar Flows",
    paperTitle: "Variational Inference with Normalizing Flows",
    venue: "ICML",
    year: 2015,
    exactDate: "2015-06-01",
    citations: 5441,
    pdf: "https://arxiv.org/pdf/1505.05770",
    openSource: "https://github.com/abdulfatir/planar-flow-pytorch",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "Binarized MNIST −ln p(x): ≤ 85.1 (DLGM+NF, K=80)",
    flowchart: "z₀ → [flow f₁] → z₁ → [flow f₂] → ... → z_K",
    highLevelIntuition: "Start with a simple latent sample.\nTransform it step-by-step with invertible flows to make the posterior more flexible.",
    pros: [
      "Allows arbitrarily flexible approximate posteriors",
      "Linear-time computation of Jacobian determinant",
      "Systematically improves performance with flow length"
    ],
    cons: [
      "May require many flow steps for complex distributions",
      "Not all parameterizations are invertible",
      "Additional computation per flow step"
    ],
    analogy: "A chef starts with a plain vegetable purée, then repeatedly kneads, twists, and folds it with new techniques—like rolling, squeezing, and marinating—each step subtly reshaping the texture and flavor, until it becomes a surprisingly complex and refined dish.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1505.05770",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "VRNN",
    model: "Variational RNN",
    paperTitle: "A Recurrent Latent Variable Model for Sequential Data (VRNN)",
    venue: "NeurIPS",
    year: 2015,
    exactDate: "2015-06-07",
    citations: 1711,
    pdf: "https://arxiv.org/pdf/1506.02216.pdf",
    openSource: "https://github.com/jych/nips2015_vrnn",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Audio",
    bestPerformance: "Blizzard test log-likelihood: ≈ 9516",
    flowchart: "xₜ₋₁, zₜ₋₁, hₜ₋₁\n    ↓\nCompute prior: [μ₀,ₜ, σ₀,ₜ] ← hₜ₋₁\n    ↓\nSample zₜ ~ N(μ₀,ₜ, σ₀,ₜ)\n    ↓\nCompute decoder params: [μₓ,ₜ, σₓ,ₜ] ← (zₜ, hₜ₋₁)\n    ↓\nGenerate xₜ ~ N(μₓ,ₜ, σₓ,ₜ)\n    ↓\nUpdate hidden state: hₜ = f(xₜ, zₜ, hₜ₋₁)",
    highLevelIntuition: "Use past info to set the prior over latent variables.\n\nSample a latent variable and generate the next output.\n\nUpdate the memory with new input and latent.",
    pros: [
      "Captures complex temporal dependencies via latent variables",
      "Handles highly structured and variable sequential data",
      "Generates cleaner, less noisy samples than RNN-GMM"
    ],
    cons: [
      "More complex and computationally demanding than standard RNNs",
      "Requires tuning and careful training",
      "Latent space interpretation is less straightforward"
    ],
    analogy: "A chef keeps a kitchen diary, and each day, recalls yesterday’s flavors and secret spices, then invents today’s meal by tasting, adjusting, and jotting new notes—each dish depends on all past creations and hidden tweaks, with every new serving refining both recipe and memory.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1506.02216",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "IWAE",
    model: "Importance Weighted Autoencoder",
    paperTitle: "Importance Weighted Autoencoders",
    venue: "ICLR",
    year: 2015,
    exactDate: "2015-09-01",
    citations: 1588,
    pdf: "https://arxiv.org/pdf/1509.00519",
    openSource: "https://github.com/yburda/iwae",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Image",
    bestPerformance: "MNIST (2 layers, k=50): NLL = –82.90",
    flowchart: "x → [encode] → q(h|x)\n      ↘        ↙\n   sample h₁...h_k\n      ↓\n[compute w_i = p(x, h_i)/q(h_i|x)]\n      ↓\n[average weights, log]\n      ↓\nL_k(x)",
    highLevelIntuition: "Encode input and draw multiple latent guesses.\nScore and weight each guess, then average for tighter likelihood.",
    pros: [
      "Tighter log-likelihood lower bound than VAE",
      "Learns richer latent representations",
      "Scales with more posterior samples (k)"
    ],
    cons: [
      "Higher computational cost for large k",
      "No KL analytic simplification for k > 1",
      "Limited improvement if posterior mismatch is too large"
    ],
    analogy: "A chef tastes a stew and, instead of relying on a single guess, prepares many different sample spoons, rates how closely each matches the original, and combines these ratings to refine the recipe, ensuring the final blend captures the true flavor far better than just one try.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1509.00519",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DCGAN",
    model: "Deep Convolutional GAN",
    paperTitle: "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
    venue: "arXiv",
    year: 2015,
    exactDate: "2015-11-19",
    citations: 20490,
    pdf: "https://arxiv.org/pdf/1511.06434",
    openSource: "https://github.com/pytorch/examples/tree/main/dcgan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR‑10 accuracy (unsupervised, features + L2‑SVM): 82.8%",
    flowchart: "flowcharts/gifs/DCGAN.gif",
    highLevelIntuition: "Noise is transformed to an image by the generator.\nThe discriminator distinguishes real images from generated ones.",
    pros: [
      "Learns high‑quality, reusable image features",
      "Enables stable GAN training with CNNs",
      "Generator supports semantic vector arithmetic"
    ],
    cons: [
      "Can still suffer from mode collapse",
      "Less effective than supervised CNNs for classification",
      "Instability when training too long"
    ],
    analogy: "A chef whips up dishes from random baskets of ingredients, trying to fool a master food critic who must spot whether each plate came from a secret recipe book or was just conjured up from scratch; over time, both chef and critic sharpen their skills until the made-up meals taste remarkably real.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1511.06434",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "VAEGAN",
    model: "VAE‑GAN Hybrid",
    paperTitle: "Autoencoding beyond pixels using a learned similarity metric",
    venue: "ICML",
    year: 2015,
    exactDate: "2015-12-31",
    citations: 2952,
    pdf: "https://arxiv.org/pdf/1512.09300.pdf",
    openSource: "https://github.com/andersbll/autoencoding_beyond_pixels",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Image",
    bestPerformance: "CelebA faces: Sharpest, most realistic samples among VAE-based models; Cosine similarity (LFW): 0.9114",
    flowchart: "x → [Enc] → z → [Dec] → x̃ → [Dis_l] → feature_loss\n                        ↘ [GAN Discrim] → real/fake loss",
    highLevelIntuition: "Encode input into latent vector.\nDecode/generate image, compare both pixel-wise and in discriminator’s feature space for loss.\nSimultaneously train GAN to distinguish real from generated images.",
    pros: [
      "Produces sharper, more realistic images than VAE",
      "Disentangles high-level visual features in latent space",
      "Allows simple attribute manipulation via latent arithmetic"
    ],
    cons: [
      "More complex, harder to train than standard VAE",
      "GAN component may be unstable",
      "Not competitive for semi-supervised classification"
    ],
    analogy: "A chef writes a flexible recipe after tasting a signature dish, then cooks from it, striving not just to match the look and basic flavor, but to fool both a picky diner and a gourmet judge who compare subtle qualities—so every remake is both true to the essence and deliciously convincing.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1512.09300",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PixRNN",
    model: "Pixel RNN",
    paperTitle: "Pixel Recurrent Neural Networks",
    venue: "ICML",
    year: 2016,
    exactDate: "2016-01-25",
    citations: 2775,
    pdf: "https://arxiv.org/pdf/1601.06760",
    openSource: "https://github.com/igul222/pixel_rnn",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 NLL: 3.00 bits/dim (Diagonal BiLSTM)",
    flowchart: "x₁ → x₂ → ... → xᵢ₋₁ → [PixelRNN] → xᵢ\n       ↑               │\n   (all previous pixels)",
    highLevelIntuition: "The model reads pixels in raster order, using all previously generated pixels as context. For each pixel, it predicts the next value using a recurrent neural network.",
    pros: [
      "Captures long-range pixel dependencies",
      "Models discrete pixel values directly",
      "Produces globally coherent and sharp images"
    ],
    cons: [
      "Slow sequential generation",
      "High memory/computation cost",
      "Less parallelizable than PixelCNN"
    ],
    analogy: "A chef prepares a mosaic salad by placing each tiny vegetable piece one after another, always remembering and tasting every piece already on the plate, so each new addition perfectly fits and balances with the entire growing arrangement.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1601.06760",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "LVAE",
    model: "Ladder VAE",
    paperTitle: "Ladder Variational Autoencoders",
    venue: "NeurIPS",
    year: 2016,
    exactDate: "2016-02-06",
    citations: 1090,
    pdf: "https://arxiv.org/pdf/1602.02282",
    openSource: "https://github.com/addtt/ladder-vae-pytorch",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Image",
    bestPerformance: "MNIST test log-likelihood (5000 IW samples): −81.74",
    flowchart: "x → [MLP] → d₁ → … → d_L\n        ↓\n      Upward pass: μ̂_{q,i}, σ̂^2_{q,i}\n        ↓\n      Downward pass: Combine with μ_{p,i}, σ^2_{p,i}\n        ↓\n      z_L → z_{L-1} → ... → z_1 → x",
    highLevelIntuition: "Pass input up a neural network to extract bottom-up signals.\n\nPass signals down, recursively merging top-down prior with data-driven likelihood at each layer.",
    pros: [
      "Utilizes deeper, more distributed latent hierarchies",
      "Consistently outperforms VAE baselines",
      "Provides tighter log-likelihood lower bounds"
    ],
    cons: [
      "Requires batch normalization and warm-up for stable training",
      "More complex inference than standard VAE",
      "Top-layer latent utilization diminishes on some datasets"
    ],
    analogy: "A chef first builds up a dish layer by layer, tasting at each level to understand deep, hidden flavors, then works downward, blending these insights with fresh ingredients at every step—so the final creation harmonizes both the subtle foundation and the vivid surface.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1602.02282",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "SSVAE",
    model: "Semi Supervised VAE",
    paperTitle: "Variational Autoencoders for Semi-supervised Text Classification",
    venue: "AAAI",
    year: 2016,
    exactDate: "2016-03-08",
    citations: 327,
    pdf: "https://arxiv.org/pdf/1603.02514",
    openSource: "https://github.com/kamenbliznashki/generative_models",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Text",
    bestPerformance: "IMDB error: 8.72% (SSVAE-II, 20K labels); 7.23% (SSVAE-II + LM pretrain)",
    flowchart: "x → [encode, y] → z ~ qφ(z|x,y)\n         ↓\n   [z, y] → Decoder RNN → x̂\n         ↑\n   qφ(y|x): classifier (for unlabeled)",
    highLevelIntuition: "Encode the text and label to produce a latent code.\nUse latent code and label at every decode step to reconstruct text, learning discriminative representations.",
    pros: [
      "Strong gains in low-label settings",
      "Works for sequential text (not just images)",
      "Compatible with pretraining for further boost"
    ],
    cons: [
      "Underperforms pretraining-based methods alone",
      "Still limited by RNN optimization challenges",
      "Requires careful decoder conditioning for label use"
    ],
    analogy: "A chef learns to recreate dishes by combining each meal’s flavors with a tag describing its style—sometimes the tag is known, sometimes guessed—using both the ingredients and the label together at every step, so each remake not only matches the taste but learns to classify the recipe’s true origin.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1603.02514",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "RealNVP",
    model: "Real NVP",
    paperTitle: "Density estimation using Real NVP",
    venue: "ICLR",
    year: 2017,
    exactDate: "2016-05-27",
    citations: 4729,
    pdf: "https://arxiv.org/pdf/1605.08803",
    openSource: "https://github.com/chrischute/real-nvp",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR‑10 bits/dim: 3.49",
    flowchart: "x₀ → [f] → z → [sample z ∼ p_Z] → x̂ = f⁻¹(z)",
    highLevelIntuition: "Data is transformed into a latent Gaussian using invertible layers.\nTo generate, sample from the Gaussian and invert the layers back to data.",
    pros: [
      "Exact and efficient log-likelihood computation",
      "Parallelizable and fast sampling",
      "Semantically meaningful latent space"
    ],
    cons: [
      "Slightly worse likelihood than PixelRNN",
      "Sometimes generates improbable samples",
      "Latent variables capture low-level features"
    ],
    analogy: "A chef skillfully dismantles a complex dish into distinct, simple flavors using reversible cooking tricks, then crafts a brand-new plate by randomly choosing pure tastes, finally retracing every step in reverse to reassemble a vibrant, original meal.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1605.08803",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "BiGAN",
    model: "Bidirectional GAN",
    paperTitle: "Adversarial Feature Learning",
    venue: "ICLR",
    year: 2016,
    exactDate: "2016-05-31",
    citations: 2716,
    pdf: "https://arxiv.org/pdf/1605.09782",
    openSource: "https://github.com/jeffdonahue/bigan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "VOC 2007 Detection mAP: 52.5%",
    flowchart: "flowcharts/gifs/BiGAN.gif",
    highLevelIntuition: "Sample z, generate x̃ with G; sample x, encode z̃ with E. Discriminator D tries to tell (x, z̃) from (x̃, z). G and E try to fool D by making these pairs indistinguishable.",
    pros: [
      "Learns bidirectional mapping (data ↔ latent)",
      "Competitive unsupervised feature learning",
      "Domain-agnostic (not tied to images)"
    ],
    cons: [
      "Reconstructions are imperfect",
      "Training is unstable (like GANs)",
      "Generator and encoder rarely exact inverses"
    ],
    analogy: "A chef invents a dish from mystery ingredients, then tries to reverse-engineer existing meals back to their secret recipes; meanwhile, a sharp-eyed food critic samples both creations and guesses whether each dish–recipe pair truly belongs together or is a clever forgery.",
    eli5AnalogyStyle: "tom&jerry",
    arxivId: "1605.09782",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "fGAN",
    model: "f-GAN",
    paperTitle: "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization",
    venue: "NeurIPS",
    year: 2016,
    exactDate: "2016-06-02",
    citations: 2124,
    pdf: "https://arxiv.org/pdf/1606.00709",
    openSource: "https://github.com/ivasiljevic/fgan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "MNIST Test Log-Likelihood (KDE): 429 nats (Pearson χ² divergence)",
    flowchart: "flowcharts/gifs/fGAN.gif",
    highLevelIntuition: "Generator produces samples from noise.\nVariational function tries to distinguish real vs. generated samples using chosen f-divergence.",
    pros: [
      "Trains generative neural samplers with any f-divergence",
      "Generalizes standard GAN to a broad family of divergences",
      "Can improve stability and flexibility in adversarial training"
    ],
    cons: [
      "Requires careful selection of divergence for best results",
      "Some divergences yield unstable training or poor sample quality",
      "Does not support exact likelihood computation or inference"
    ],
    analogy: "A chef makes up dishes from random pantry grabs, while a tasting panel judges them by any chosen flavor rule—be it sweetness, bitterness, or richness—letting the chef refine recipes using whatever taste metric the panel decides for that meal.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1606.00709",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "InfoGAN",
    model: "InfoGAN",
    paperTitle: "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
    venue: "NeurIPS",
    year: 2016,
    exactDate: "2016-06-12",
    citations: 6053,
    pdf: "https://arxiv.org/pdf/1606.03657",
    openSource: "https://github.com/openai/InfoGAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "5% MNIST digit classification error (unsupervised, c₁ as classifier)",
    flowchart: "z, c → [G] → x → [Q] → ĉ\n         ↓         ↑\n     maximize mutual info",
    highLevelIntuition: "Generator takes noise and code to produce a sample.\nAuxiliary network tries to recover the code from the sample, pushing the generator to encode meaningful factors.",
    pros: [
      "Learns disentangled, interpretable representations",
      "Fully unsupervised (no labels required)",
      "Adds negligible computation over GAN"
    ],
    cons: [
      "Performance sensitive to hyperparameters (λ)",
      "Training can inherit GAN instability",
      "Interpretability depends on code choice/design"
    ],
    analogy: "A chef invents new dishes by mixing both random ingredients and secret cues—like “add crunch” or “make it colorful”—then a food detective samples the plate and tries to guess each hidden cue, encouraging the chef to pack every subtle instruction into the flavors for easy discovery.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1606.03657",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "IAF",
    model: "Inverse Autoregressive Flow",
    paperTitle: "Improving Variational Autoencoders with Inverse Autoregressive Flow",
    venue: "ICML",
    year: 2016,
    exactDate: "2016-06-19",
    citations: 2367,
    pdf: "https://arxiv.org/pdf/1606.04934",
    openSource: "https://github.com/openai/iaf",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10: 3.11 bits/dim",
    flowchart: "x → [Encoder NN] → (μ₀, σ₀, h)\n         ↓\n      ε ∼ N(0, I)\n         ↓\n  z₀ = μ₀ + σ₀ ⊙ ε\n         ↓\n [for t = 1...T:]\n      ↓\n [Autoregressive NN: (mₜ, sₜ)]\n      ↓\n  σₜ = sigmoid(sₜ)\n      ↓\n zₜ = σₜ ⊙ zₜ₋₁ + (1−σₜ) ⊙ mₜ\n         ↓\n      z_T",
    highLevelIntuition: "Start with a simple latent sample from the encoder.\nApply a sequence of autoregressive, invertible transforms to increase posterior flexibility.",
    pros: [
      "Scales efficiently to high-dimensional latent spaces",
      "Enables highly flexible approximate posteriors",
      "Allows much faster sampling than autoregressive decoders"
    ],
    cons: [
      "Complexity increases with flow depth",
      "Requires careful parameterization for stability",
      "More computation vs. simple diagonal posteriors"
    ],
    analogy: "A chef begins with a basic sauce, then repeatedly refines it by tasting and, at each step, cleverly adjusting spices and textures based on everything already added, layering in new flavors with each pass until the final blend is richer and far more nuanced than the simple starting point.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1606.04934",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PixCNN",
    model: "Pixel CNN",
    paperTitle: "Pixel Recurrent Neural Networks",
    venue: "ICML",
    year: 2016,
    exactDate: "2016-06-19",
    citations: 2775,
    pdf: "https://arxiv.org/pdf/1601.06759",
    openSource: "https://github.com/igul222/pixel_rnn",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 NLL Test: 3.14 bits/dim",
    flowchart: "x₁, ..., x_{i-1} → [masked convs] → predict p(x_i | x_{<i}) → sample x_i",
    highLevelIntuition: "The model scans the image left-to-right, top-to-bottom.\nFor each pixel, it uses convolutions over already-generated pixels to predict the next value.",
    pros: [
      "Fully convolutional, parallelizable during training",
      "Captures spatial pixel dependencies without latent variables",
      "Simpler and faster than RNN-based models"
    ],
    cons: [
      "Limited receptive field compared to PixelRNN",
      "Slower sequential sampling at generation time",
      "Struggles with long-range structure"
    ],
    analogy: "A chef constructs a pie one slice at a time, using a special cutter that only lets each new slice touch and blend with the already-baked portions, ensuring every piece tastes right with what’s already there—until the full pie is done.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1601.06759",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "WaveNet",
    model: "WaveNet",
    paperTitle: "WaveNet: A Generative Model for Raw Audio",
    venue: "arXiv",
    year: 2016,
    exactDate: "2016-09-12",
    citations: 6537,
    pdf: "https://arxiv.org/pdf/1609.03499.pdf",
    openSource: "https://github.com/ibab/tensorflow-wavenet",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Audio",
    bestPerformance: "MOS (naturalness): 4.21 (English), 4.08 (Mandarin)",
    flowchart: "x₁, ..., x_{t−1} → [dilated causal convs] → [softmax] → x_t",
    highLevelIntuition: "Each new audio sample is predicted from all previous samples using a stack of dilated convolutions.\nThe output is a probability distribution; the sample is then generated from this distribution.",
    pros: [
      "Generates highly natural raw audio",
      "Captures long-range temporal dependencies",
      "Flexible: supports multi-speaker, TTS, music"
    ],
    cons: [
      "Sequential (slow) generation",
      "High computational cost",
      "Limited receptive field for long-term consistency"
    ],
    analogy: "A chef builds a layered soup, where every spoonful’s seasoning depends on all prior ladles, using increasingly spaced-out taste tests so each new addition harmonizes with subtle echoes of the entire soup’s past flavors, one ingredient at a time.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1609.03499",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "ACGAN",
    model: "Auxiliary Classifier GAN",
    paperTitle: "Conditional Image Synthesis with Auxiliary Classifier GANs (AC-GAN)",
    venue: "NeurIPS",
    year: 2017,
    exactDate: "2016-10-30",
    citations: 4578,
    pdf: "https://arxiv.org/pdf/1610.09585",
    openSource: "https://github.com/eriklindernoren/Keras-GAN?tab=readme-ov-file#ac-gan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 Inception Score: 8.25 ± 0.07",
    flowchart: "flowcharts/gifs/AC-GAN.gif",
    highLevelIntuition: "Sample z and class c, generator produces a fake image.\nDiscriminator checks if image is real/fake and predicts class.",
    pros: [
      "Generates globally coherent, high-res (128×128) images",
      "Achieves state-of-the-art CIFAR-10 Inception score (at time)",
      "Maintains class-conditional sample diversity"
    ],
    cons: [
      "Lower visual discriminability than real data",
      "Only 84.7% of classes match real data diversity",
      "Requires splitting classes for stable training"
    ],
    analogy: "A chef receives both a mystery basket and a menu request—say, “make something spicy”—whips up a dish, and then a judge not only decides if it’s genuine or made up, but also tries to guess which menu category it fits, rewarding creations that taste both authentic and true to their claimed style.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1610.09585",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "βVAE",
    model: "β‑VAE",
    paperTitle: "β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
    venue: "ICLR",
    year: 2017,
    exactDate: "2016-11-04",
    citations: 6439,
    pdf: "https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/generative/disentangled-representations/beta-vae,-higgins,-iclr2017.pdf",
    openSource: "https://github.com/1Konny/Beta-VAE",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Image",
    bestPerformance: "Disentanglement metric: 99.23% on 2D Shapes dataset",
    flowchart: "x → [encoder] → z → [sample] → z' → [decoder] → x'",
    highLevelIntuition: "Encode an image into a compressed, factorized latent space.\nDecode from this space, learning to separate independent generative factors.",
    pros: [
      "Learns highly disentangled representations",
      "Fully unsupervised; no prior on factor number/type required",
      "Stable and robust to architectures and datasets"
    ],
    cons: [
      "Blurry reconstructions at high β",
      "Some minor factors may be lost at very high β",
      "Requires β tuning per dataset/architecture"
    ],
    analogy: "A chef invents a method where every element of a dish—crunch, color, aroma, spice—must be clearly separated and tasted on its own, then combines these purified flavors back into a single plate, even if the end result is a bit softer or blurrier than the original meal.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "GMVAE",
    model: "Gaussian Mixture VAE",
    paperTitle: "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders",
    venue: "arXiv",
    year: 2016,
    exactDate: "2016-11-08",
    citations: 1035,
    pdf: "https://arxiv.org/pdf/1611.02648",
    openSource: "https://github.com/jariasf/GMVAE",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Image",
    bestPerformance: "MNIST clustering accuracy (K=16, M=10): 96.92%",
    flowchart: "w ~ N(0, I)\n    ↓\nz ~ Mult(π)\n    ↓\nx ~ N(μ_{z}(w;β), σ²_{z}(w;β))\n    ↓\ny ~ pθ(y|x)",
    highLevelIntuition: "Sample w to set up the overall style of possible clusters.\n\nPick a cluster label z.\n\nGenerate a latent x from that cluster’s Gaussian.\n\nDecode x into an observation y.",
    pros: [
      "Learns distinct, interpretable clusters in an unsupervised manner",
      "Supports multimodal latent distributions for complex data",
      "Competitive with state-of-the-art clustering methods"
    ],
    cons: [
      "Suffers from over-regularization and cluster degeneracy without heuristics",
      "Sensitive to initialization and number of clusters/samples",
      "Requires tuning of additional hyperparameters (e.g., minimum information constraint)"
    ],
    analogy: "A chef first selects a secret sauce base to set the overall mood of a banquet, then randomly picks a themed table, prepares a dish using flavors unique to that table, and finally serves up a finished plate that reveals which culinary club it belongs to—all without ever seeing the guest list.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1611.02648",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PixVAE",
    model: "Pixel VAE",
    paperTitle: "PixelVAE: A Latent Variable Model for Natural Images",
    venue: "ICLR",
    year: 2017,
    exactDate: "2016-11-15",
    citations: 426,
    pdf: "https://arxiv.org/pdf/1611.05013",
    openSource: "https://github.com/igul222/PixelVAE",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Image",
    bestPerformance: "Binarized MNIST NLL ≤ 79.02",
    flowchart: "x₀ → [encoder] → z → [PixelCNN decoder, conditioned on z] → x̂",
    highLevelIntuition: "The encoder compresses the image into a global latent code.\n\nThe decoder uses this code to guide PixelCNN layers that generate detailed pixels.",
    pros: [
      "Combines global structure modeling of VAE with fine-detail generation of PixelCNN",
      "Requires fewer autoregressive layers than PixelCNN",
      "Learns compressed, meaningful latent representations"
    ],
    cons: [
      "Sampling and training still slower than plain VAE",
      "Some complexity from hierarchical extension",
      "Slightly lower likelihood than Gated PixelCNN on ImageNet"
    ],
    analogy: "A chef first writes a quick summary recipe capturing the essence of a grand feast, then uses that summary to guide a series of careful, stepwise touches—adding fine garnishes and precise seasonings one at a time—so the finished dish mirrors both the big idea and every intricate detail.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1611.05013",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "Pix2Pix",
    model: "pix2pix",
    paperTitle: "Image-to-Image Translation with Conditional Adversarial Networks",
    venue: "CVPR",
    year: 2017,
    exactDate: "2016-11-16",
    citations: 27994,
    pdf: "https://arxiv.org/pdf/1611.07004",
    openSource: "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "AMT “real vs fake” (Aerial→Map): 18.9% fooled",
    flowchart: "x → [G] → G(x, z) → [D(x, G(x, z))] → Real/Fake",
    highLevelIntuition: "Input image passes through generator to synthesize a target-style image.\nDiscriminator checks if the generated pair looks real or fake.\nBoth networks train adversarially to improve realism and fidelity.",
    pros: [
      "Generic, application-agnostic image-to-image translation",
      "Produces sharper, more realistic outputs than L1/L2 losses",
      "Works with small datasets and fast to train/infer"
    ],
    cons: [
      "Fails on highly ambiguous or sparse inputs",
      "Can introduce artifacts and hallucinated structures",
      "Struggles with highly stochastic or multimodal targets"
    ],
    analogy: "A chef receives a starter dish—say, a plain omelet—and must transform it into a gourmet version by clever seasoning and plating, while a sharp judge compares before-and-after plates to decide if the upgrade truly looks and tastes like an authentic chef’s touch or is just a dressed-up fake.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1611.07004",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "NADE",
    model: "Neural Autoregressive Distribution Estimation",
    paperTitle: "Neural Autoregressive Distribution Estimation",
    venue: "JMLR",
    year: 2016,
    exactDate: "2016-11-21",
    citations: 449,
    pdf: "https://arxiv.org/pdf/1605.02226",
    openSource: "https://github.com/MarcCote/NADE",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Tabular",
    bestPerformance: "Binarized MNIST: −80.82 nats (ConvNADE+DeepNADE, Table 4)",
    flowchart: "x₀ → [mask previous dims] → h₁ → [feedforward NN] → p(x₁|·) → ... → x_D",
    highLevelIntuition: "Each variable is predicted given all previous ones.\nA neural network learns the conditional distribution for each variable, sharing parameters across steps.",
    pros: [
      "Tractable and efficient likelihood computation",
      "Competitive with deep/intractable generative models",
      "Extensible to real-valued data and convolutional architectures"
    ],
    cons: [
      "Fixed variable ordering can limit conditional inference",
      "Deep versions increase computational cost",
      "Less effective on high-dimensional data without structural priors"
    ],
    analogy: "A chef assembles a layered trifle, adding each new ingredient only after carefully tasting all the previous layers, with a master recipe that guides every choice so each step’s flavor depends on—and is predicted from—what’s already in the bowl.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1605.02226",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "SRGAN",
    model: "Super Resolution GAN",
    paperTitle: "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
    venue: "CVPR",
    year: 2017,
    exactDate: "2016-11-24",
    citations: 15350,
    pdf: "https://arxiv.org/pdf/1609.04802",
    openSource: "https://github.com/tensorlayer/srgan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "MOS (Mean Opinion Score) on BSD100: 3.56 (state-of-the-art for 4× upscaling)",
    flowchart: "I_{LR} → [Generator G] → I_{SR} → [Discriminator D] → [Adversarial & Perceptual Losses] → G",
    highLevelIntuition: "Input a low-resolution image.\nGenerator creates a super-resolved guess.\nDiscriminator critiques realism.\nLoss combines human-perception features and fooling the discriminator.",
    pros: [
      "Recovers fine texture details for large upscaling factors",
      "Generates perceptually photo-realistic images",
      "Outperforms prior methods in human perceptual studies"
    ],
    cons: [
      "Lower PSNR/SSIM compared to MSE-based models",
      "May introduce minor high-frequency artifacts",
      "Training is more difficult and less stable than standard SR models"
    ],
    analogy: "A chef is handed a tiny, blurry appetizer and must reconstruct it as a full, lavish entrée—relying on both their sense of gourmet realism and the critiques of a fussy food judge—until every recreated plate not only looks but tastes like an authentic culinary masterpiece.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1609.04802",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PixCNN++",
    model: "PixelCNN++",
    paperTitle: "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications",
    venue: "ICLR",
    year: 2017,
    exactDate: "2017-01-19",
    citations: 1284,
    pdf: "https://arxiv.org/pdf/1701.05517",
    openSource: "https://github.com/openai/pixel-cnn",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10: 2.92 bits per sub-pixel",
    flowchart: "x₀ → [mixture comp.] → ν → [rounding] → x",
    highLevelIntuition: "Each pixel is modeled using a flexible mixture of continuous distributions.\nOutput is quantized and dependencies are captured via convolutional structure.",
    pros: [
      "State-of-the-art log-likelihood on CIFAR-10",
      "Faster training via discretized logistic mixture likelihood",
      "Improved optimization and regularization (dropout, skip connections)"
    ],
    cons: [
      "Still slower sampling compared to non-autoregressive models",
      "Performance sensitive to network depth/field of view",
      "Increased architectural complexity over original PixelCNN"
    ],
    analogy: "A chef builds a cake by crafting each slice using a blend of several cream and fruit mixtures, precisely adjusting and smoothing every new piece so it fits perfectly with the flavors of the earlier ones—layering up the dessert slice by slice until the whole cake is seamless and rich.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1701.05517",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "LSGAN",
    model: "Least Squares GAN",
    paperTitle: "Least Squares Generative Adversarial Networks",
    venue: "ICCV",
    year: 2017,
    exactDate: "2017-01-26",
    citations: 6659,
    pdf: "https://arxiv.org/pdf/1611.04076",
    openSource: "https://github.com/subpath/LSGAN-with-PyTorch",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Higher quality images than DCGAN/EBGAN on LSUN Bedroom",
    flowchart: "z → G(z) → x_fake\nx_real, x_fake → D(x)\nD(x) → [least squares loss] → update D, G\n",
    highLevelIntuition: "Noise is mapped to images by the generator.\nDiscriminator assigns scores using least squares loss, updating both networks.",
    pros: [
      "Generates more realistic images than regular GANs",
      "More stable training (less mode collapse)",
      "Relieves vanishing gradients problem"
    ],
    cons: [
      "Evaluated mainly on scene datasets and Chinese characters",
      "Stability not perfect in all optimizer settings",
      "Only slightly improves over DCGANs in some conditions"
    ],
    analogy: "A chef creates new dishes from surprise baskets, while a judge rates each plate not as simply real or fake, but by giving it a score on a sliding taste scale—so both chef and judge steadily refine their creations, aiming for dishes that land in the sweet spot of truly delicious.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1611.04076",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "WGAN",
    model: "Wasserstein GAN",
    paperTitle: "Wasserstein GAN",
    venue: "ICML",
    year: 2017,
    exactDate: "2017-01-26",
    citations: 19454,
    pdf: "https://arxiv.org/pdf/1701.07875",
    openSource: "https://github.com/martinarjovsky/WassersteinGAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Meaningful loss correlates with sample quality on LSUN Bedrooms",
    flowchart: "z  →  gθ(z)  →  x̂\n             ↓\nReal x ──→ Critic f_w ──→ Wasserstein loss\n",
    highLevelIntuition: "Noise z is transformed by the generator into fake data.\nCritic scores real and fake data, and generator improves to fool the critic.\nCritic is trained to maximize the difference between real and generated scores.\nGenerator is updated to minimize this difference.",
    pros: [
      "Stable training, no need for careful balance between generator and discriminator",
      "Loss metric correlates with sample quality",
      "Reduces mode collapse compared to standard GANs"
    ],
    cons: [
      "Requires enforcing Lipschitz constraint (weight clipping is crude)",
      "Can be unstable with Adam/momentum-based optimizers on the critic",
      "Loss scale depends on critic architecture, limiting cross-model comparison"
    ],
    analogy: "A chef invents dishes from random ingredients, while a critic gives each plate a precise score reflecting how close it tastes to authentic cuisine—not just pass/fail—so both chef and critic continuously push the recipes toward higher, truly gourmet standards.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1701.07875",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DiscoGAN",
    model: "Discover GAN",
    paperTitle: "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks",
    venue: "ICML",
    year: 2017,
    exactDate: "2017-03-15",
    citations: 2842,
    pdf: "https://arxiv.org/pdf/1703.05192",
    openSource: "https://github.com/SKTBrain/DiscoGAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Car-to-car translation: Preserves azimuth correlation, avoids mode collapse (Figure 5c)",
    flowchart: "x_A → [G_AB] → x_AB → [G_BA] → x_ABA\nx_B → [G_BA] → x_BA → [G_AB] → x_BAB\n",
    highLevelIntuition: "Translate an image to the other domain and back.\nForce the reconstruction to match the input, ensuring true cross-domain relation.",
    pros: [
      "Learns cross-domain relations from unpaired data",
      "Prevents mode collapse with bijective mapping",
      "Works across visually distinct domains"
    ],
    cons: [
      "Limited to domains with at least one shared feature",
      "Requires two discriminators and generators",
      "May fail with highly multimodal/ambiguous mappings"
    ],
    analogy: "A chef is challenged to turn an Italian pasta into a Chinese noodle dish, then transform it back again, ensuring the final plate tastes just like the original—proving true mastery in translating flavors between distinct cuisines while never losing the dish’s unique soul.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1703.05192",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "CycGAN",
    model: "Cycle GAN",
    paperTitle: "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
    venue: "ICCV",
    year: 2017,
    exactDate: "2017-03-27",
    citations: 28442,
    pdf: "https://arxiv.org/pdf/1703.10593",
    openSource: "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "AMT “real vs fake” (Map→Photo): 26.8% ± 2.8% fool rate (256×256)",
    flowchart: "flowcharts/gifs/CycGAN.gif",
    highLevelIntuition: "Translate an image to the target domain.\nThen translate it back—if cycle-consistency holds, it matches the original.",
    pros: [
      "Works without paired training data",
      "Enables diverse image-to-image translation tasks",
      "Preserves structure/content via cycle consistency"
    ],
    cons: [
      "Limited in handling large geometric transformations",
      "Sometimes fails on complex object mappings",
      "Slightly worse than supervised methods with paired data"
    ],
    analogy: "A chef takes a French ratatouille and transforms it into a spicy Indian curry, then reverses the process to recreate the original ratatouille—ensuring each round-trip preserves every essential ingredient and flavor, even though no recipe pairs were given.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1703.10593",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "WGAN-GP",
    model: "WGAN‑Gradient Penalty",
    paperTitle: "Improved Training of Wasserstein GANs",
    venue: "NeurIPS",
    year: 2017,
    exactDate: "2017-03-31",
    citations: 13523,
    pdf: "https://arxiv.org/pdf/1704.00028.pdf",
    openSource: "https://github.com/igul222/improved_wgan_training",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 Inception Score (unsupervised): 7.86±0.07",
    flowchart: "z → G(z) → x̃\nx , x̃ → [ε-mix] → x̂ → [∇x̂ D(x̂)]\nx , x̃ , x̂ → D → Loss\n",
    highLevelIntuition: "Generate a fake sample from noise. Interpolate between real and fake samples. Penalize the critic’s gradient at these points to enforce Lipschitzness and train both networks.\n",
    pros: [
      "Stable training across many GAN architectures",
      "Removes need for weight clipping and batch normalization in critic",
      "Achieves state-of-the-art results on standard benchmarks"
    ],
    cons: [
      "Slightly slower convergence in wall-clock time vs. DCGAN",
      "May still overfit with large critic and limited data",
      "Performance sensitive to penalty coefficient λ"
    ],
    analogy: "A chef creates new dishes from mystery baskets, while the judge not only scores authenticity but also checks how smoothly flavors change when blending real and invented plates—rewarding creations where every transition tastes natural and never jarring, no matter the mix.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1704.00028",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "MAF",
    model: "Masked Autoregressive Flow",
    paperTitle: "Masked Autoregressive Flow for Density Estimation",
    venue: "NeurIPS",
    year: 2017,
    exactDate: "2017-05-19",
    citations: 1783,
    pdf: "https://arxiv.org/pdf/1705.07057",
    openSource: "https://github.com/kamenbliznashki/normalizing_flows/blob/master/maf.py",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Tabular",
    bestPerformance: "BSDS300 log-likelihood: 156.36 nats (MAF MoG, single model, Table 1)",
    flowchart: "u ~ N(0, I) \n   ↓\n[Autoregressive layer(s): u → x via x_i = u_i exp(α_i) + μ_i]\n   ↓\nx\n",
    highLevelIntuition: "Random numbers are mapped to data through a series of masked autoregressive neural nets. Each layer adds flexibility, modeling complex dependencies in the data.",
    pros: [
      "State-of-the-art density estimation on several benchmarks",
      "Tractable and exact density evaluation",
      "More flexible than coupling-layer flows (e.g., Real NVP)"
    ],
    cons: [
      "Slow sampling (requires sequential generation)",
      "Lacks image-specific inductive biases",
      "Higher training cost than simple autoregressive models"
    ],
    analogy: "A chef starts with scoops of plain base flavors, then sequentially blends in herbs and spices using a series of custom stepwise mixers—each stage tasting and adjusting every new addition based on all previous blends—until the final dish achieves layers of subtle, interdependent complexity.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1705.07057",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "VEEGAN",
    model: "Variational Encoder Enhancement GAN",
    paperTitle: "VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning",
    venue: "NeurIPS",
    year: 2017,
    exactDate: "2017-05-22",
    citations: 921,
    pdf: "https://arxiv.org/pdf/1705.07761.pdf",
    openSource: "https://github.com/akashgit/VEEGAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Stacked MNIST: 150 modes (Max 1000), best among compared GANs",
    flowchart: "z ~ N(0, I) → [Gγ] → x̂\nx̂ → [Fθ] → ẑ\nx (real) → [Fθ] → z'\n",
    highLevelIntuition: "Sample noise z, generate x_hat using the generator.\nThe reconstructor tries to recover z from x_hat, and also maps real x to look like noise.",
    pros: [
      "Dramatically reduces mode collapse",
      "Does not require data-space reconstruction loss",
      "Consistently produces higher quality samples than prior GANs"
    ],
    cons: [
      "Requires an additional reconstructor network",
      "More complex objective than standard GAN",
      "Sample quality may still lag behind supervised benchmarks"
    ],
    analogy: "A chef invents dishes from random mystery blends, then asks a second chef to taste each creation and guess the exact secret ingredients; this process keeps both chefs honest, preventing them from making the same dish over and over, while encouraging a diverse and surprising menu.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1705.07761",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DrNet",
    model: "Disentangled Representation Network",
    paperTitle: "Unsupervised Learning of Disentangled Representations from Video (DRNET)",
    venue: "NeurIPS",
    year: 2017,
    exactDate: "2017-05-31",
    citations: 882,
    pdf: "https://arxiv.org/pdf/1705.10915",
    openSource: "https://github.com/edenton/drnet",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Video",
    bestPerformance: "KTH video generation: Inception Score decays less than MCNet for 100+ future steps",
    flowchart: "flowcharts/gifs/DrNet.gif",
    highLevelIntuition: "Frame is split into content (what) and pose (how it moves).\nContent stays fixed; pose changes over time and is predicted forward.\nDecoder combines both to reconstruct or generate the next frame.",
    pros: [
      "Clean disentanglement of content and pose",
      "Coherent, long-range video generation (hundreds of steps)",
      "Simple, interpretable latent space"
    ],
    cons: [
      "Relies on aggressive adversarial loss tuning",
      "Assumes pose is not clip-distinctive",
      "Pixel-space reconstruction can be blurry for complex scenes"
    ],
    analogy: "A chef separates a meal into its core ingredients and serving style—like flavor versus plating—then holds the flavors steady while imagining new ways to present and move the dish, blending both to plate up future servings that keep the essence but change the experience.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1705.10915",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "T",
    model: "Transformer",
    paperTitle: "Attention Is All You Need",
    venue: "NeurIPS",
    year: 2017,
    exactDate: "2017-06-12",
    citations: 187169,
    pdf: "https://arxiv.org/pdf/1706.03762",
    openSource: "https://github.com/jadore801120/attention-is-all-you-need-pytorch",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "WMT14 En-De BLEU: 28.4",
    flowchart: "x → [embed + pos] → encoder (self-attn + FFN) → z\nz, y_{<t} → decoder (masked self-attn + cross-attn + FFN) → y_t\n",
    highLevelIntuition: "Embed inputs and add position info.\nEncode sequence using only self-attention (captures all relationships at once).\nDecode each output step, attending to all input and past output positions.",
    pros: [
      "Fully parallelizable; fast training",
      "Superior long-range dependency modeling",
      "Outperforms previous SOTA on translation"
    ],
    cons: [
      "O(n^2) memory/computation per sequence",
      "Reduced effective resolution for long sequences",
      "Needs positional encoding to model order"
    ],
    analogy: "A chef gathers all ingredients on the counter, scans and sniffs every item at once—regardless of order—to instantly decide how each should pair with every other, then plates the meal step by step, always recalling the whole spread and previous choices with every new addition.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1706.03762",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "MCNet",
    model: "Motion Control Network",
    paperTitle: "Decomposing Motion and Content for Natural Video Sequence Prediction",
    venue: "CVPR",
    year: 2017,
    exactDate: "2017-06-25",
    citations: 755,
    pdf: "https://arxiv.org/pdf/1706.08033",
    openSource: "https://github.com/rubenvillegas/iclr2017mcnet",
    type: "Other",
    color: "#a855f7",
    primaryUseCase: "Video",
    bestPerformance: "UCF-101: Highest SSIM among all models for frames with significant motion",
    flowchart: "x₁…x_t → [x_t − x_{t−1}] → Motion Encoder (d_t)\n           │\n           ↓\n      x_t → Content Encoder (s_t)\n           │\n           ↓\n[d_t, s_t, r_t] → Combination & Decoder → x̂_{t+1}\n",
    highLevelIntuition: "Compute difference images to capture motion.\nExtract spatial features from the last frame.\nFuse motion and content features to predict the next frame.",
    pros: [
      "Separates motion and content for better video prediction",
      "End-to-end trainable with no extra supervision",
      "Outperforms previous state-of-the-art on multiple video datasets"
    ],
    cons: [
      "Slightly worse on videos with minimal motion (copy-last-frame baseline is strong there)",
      "Prediction quality degrades for very long sequences",
      "Sensitive to large camera motion"
    ],
    analogy: "A chef compares today’s dish to yesterday’s, noting every subtle change in presentation, then separately analyzes the core flavors and the new twists, finally blending both the changes and the main recipe to serve up a fresh plate that feels both familiar and excitingly new.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1706.08033",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "MoCoGAN",
    model: "Motion Control GAN",
    paperTitle: "MoCoGAN: Decomposing Motion and Content for Video Generation",
    venue: "CVPR",
    year: 2018,
    exactDate: "2017-07-16",
    citations: 1474,
    pdf: "https://arxiv.org/pdf/1707.04993",
    openSource: "https://github.com/sergeytulyakov/mocogan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Video",
    bestPerformance: "UCF101 Inception Score: 12.42±0.03",
    flowchart: "z_C → [fixed] ┐\n              │\n[ε^(1)] → R_M → z_M^(1) ─┐\n[ε^(2)] → R_M → z_M^(2) ─┼─> [concat] → G_I → x̃^(1), x̃^(2), ..., x̃^(K)\n ...                      │\n[ε^(K)] → R_M → z_M^(K) ─┘\n",
    highLevelIntuition: "Sample a content code for identity, and run a recurrent net over noise to produce motion codes. Concatenate content and motion at each timestep, pass to a generator, and produce each video frame.",
    pros: [
      "Separates motion and content for controllable video generation",
      "Handles variable video lengths",
      "Generates more realistic and consistent videos than prior GANs"
    ],
    cons: [
      "Quality sensitive to balance of content/motion latent dimensions",
      "Motion realism limited by RNN expressiveness",
      "Training is complex (multiple discriminators, alternating steps)"
    ],
    analogy: "A chef selects a signature ingredient to define a dish’s identity, then, for each course in a tasting menu, improvises unique presentation styles using a series of inspired gestures—blending the constant core flavor with ever-changing flair to craft a lively, coordinated meal.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1707.04993",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DualGAN",
    model: "Dual GAN",
    paperTitle: "DualGAN: Unsupervised Dual Learning for Image-to-Image Translation",
    venue: "ICCV",
    year: 2017,
    exactDate: "2017-10-22",
    citations: 2818,
    pdf: "https://arxiv.org/pdf/1704.02510",
    openSource: "https://github.com/duxingren14/DualGAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Outperforms cGAN on AMT “realness” for sketch→photo (1.87 vs. 1.69) and day→night (2.42 vs. 1.89)",
    flowchart: "flowcharts/gifs/DualGAN.gif",
    highLevelIntuition: "Each input is translated to the opposite domain.\nThe translation is then mapped back to reconstruct the original input.",
    pros: [
      "Unsupervised: requires no paired training data",
      "Outperforms GAN/cGAN on some perceptual tasks",
      "Preserves sharpness and structural details"
    ],
    cons: [
      "Inferior to cGAN on tasks needing pixel-wise label mapping",
      "May map pixels to wrong labels without correspondence",
      "Lags behind on segmentation accuracy"
    ],
    analogy: "A chef transforms a rustic bread into a fancy cake, then reverses the process to turn it back into bread—making sure every transformation, though wild and unscripted, still lets you recognize and savor the original’s true flavor on the return trip.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1704.02510",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "ProgGAN",
    model: "Progressive GAN",
    paperTitle: "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
    venue: "ICLR",
    year: 2018,
    exactDate: "2017-10-27",
    citations: 10119,
    pdf: "https://arxiv.org/pdf/1710.10196",
    openSource: "https://github.com/facebookresearch/pytorch_GAN_zoo",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR10 Inception Score (unsupervised): 8.80",
    flowchart: "z → [G_4×4] → ... → [G_1024×1024] → x_fake  \nx_real → [D_1024×1024] → ... → [D_4×4] → score\n",
    highLevelIntuition: "Start with tiny images, train to synthesize and discriminate at low resolution.\nProgressively add layers, increasing detail, until reaching high-res images.",
    pros: [
      "Stable training at high resolutions",
      "Faster convergence (up to 5.4× speedup)",
      "High-quality, high-variation image synthesis"
    ],
    cons: [
      "Requires careful memory management (mini-batch size reduction at high res)",
      "LSGAN loss less stable than WGAN-GP",
      "Still imperfect semantic understanding (some artifacts/unnatural shapes)"
    ],
    analogy: "A chef first perfects tiny tasting bites of a new dish, then gradually scales up—layer by layer—refining and expanding each step until the final creation dazzles as a full, high-resolution feast bursting with intricate flavors and textures.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1710.10196",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "VQVAE",
    model: "VQ‑VAE",
    paperTitle: "Neural Discrete Representation Learning",
    venue: "NeurIPS",
    year: 2017,
    exactDate: "2017-11-02",
    citations: 6565,
    pdf: "https://arxiv.org/pdf/1711.00937.pdf",
    openSource: "https://github.com/ritheshkumar95/pytorch-vqvae",
    type: "VAE",
    color: "#facc15",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR10: 4.67 bits/dim (comparable to deep conv. VAE at 4.54 bits/dim)",
    flowchart: "x → [encoder] → z_e(x) → [nearest embedding] → z_q(x) → [decoder] → x̂",
    highLevelIntuition: "Encode input into continuous vector.\nSnap to nearest codebook entry (discrete bottleneck).\nDecode using only the quantized code.",
    pros: [
      "Discrete latents avoid posterior collapse",
      "Performs nearly as well as continuous VAEs",
      "Unsupervised learning of high-level features (e.g., phonemes, objects)"
    ],
    cons: [
      "Slightly blurrier reconstructions than originals",
      "Quantization step is non-differentiable (needs straight-through estimator)",
      "Training autoregressive prior is a separate stage"
    ],
    analogy: "A chef finely chops vegetables, then for each piece, snaps it to the closest shape from a tray of standard molds before assembling the salad—so every serving is made from a set of familiar, perfectly-shaped bites, even if the original cuts were a little smoother.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1711.00937",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "StarGAN",
    model: "Star GAN",
    paperTitle: "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation",
    venue: "CVPR",
    year: 2018,
    exactDate: "2017-11-27",
    citations: 5113,
    pdf: "https://arxiv.org/pdf/1711.09020",
    openSource: "https://github.com/yunjey/stargan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "AMT single-attribute transfer (CelebA): 66.2% (hair color), 39.1% (gender), 70.6% (aged) preference",
    flowchart: "x + c → [G] → G(x, c) → [D] → (real/fake, domain)\n          ↓\n        [G, c']\n          ↓\n     reconstruction x̂\n",
    highLevelIntuition: "Add target label c to input image x and generate a translated image.\nDiscriminator checks realism and correct domain; generator reconstructs original to ensure content is preserved.",
    pros: [
      "Handles multi-domain translation with a single model",
      "Requires fewer parameters than separate models",
      "Preserves identity and visual quality well"
    ],
    cons: [
      "Struggles if domain labels are incomplete",
      "May be less effective on domains with limited data",
      "Requires correct mask vector setup for multi-dataset"
    ],
    analogy: "A chef receives a dish and a tag for the new style—like “make it spicy” or “change cuisine”—transforms the meal accordingly, and then double-checks both that the makeover looks genuine and that they can still recreate the original dish from the transformed plate, all with one versatile kitchen.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1711.09020",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PrlWaveNet",
    model: "Parallel WaveNet",
    paperTitle: "Parallel WaveNet: Fast High-Fidelity Speech Synthesis",
    venue: "ICML",
    year: 2018,
    exactDate: "2017-11-28",
    citations: 1062,
    pdf: "https://arxiv.org/pdf/1711.10433",
    openSource: "https://github.com/PhilippeNguyen/keras_wavenet",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Audio",
    bestPerformance: "MOS: 4.41 (24kHz, 16-bit, English speaker, matches autoregressive WaveNet)",
    flowchart: "z  → [IAF1] → x₁  → [IAF2] → x₂  → ... → [IAFN] → x_N\n        ↓\n    (teacher scores samples)\n",
    highLevelIntuition: "Noise is transformed in parallel into speech using stacked flows.\nA pre-trained WaveNet guides the flows to match natural speech statistics.",
    pros: [
      "Over 1000× faster than autoregressive WaveNet",
      "High-fidelity, human-like speech synthesis",
      "Supports multi-speaker and multi-language output"
    ],
    cons: [
      "Training requires pre-trained autoregressive WaveNet",
      "Quality depends on distillation process",
      "Slightly more complex training pipeline"
    ],
    analogy: "A chef takes a pile of random ingredients and, with the guidance of a master’s recipe book, quickly runs them through a series of specialized blenders in parallel—instantly whipping up a banquet where every dish tastes just as rich and authentic as if crafted one by one.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1711.10433",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "BiCycGAN",
    model: "Bidirectional Cycle GAN",
    paperTitle: "Toward Multimodal Image-to-Image Translation",
    venue: "NeurIPS",
    year: 2018,
    exactDate: "2017-11-30",
    citations: 2064,
    pdf: "https://arxiv.org/pdf/1711.11586",
    openSource: "https://github.com/junyanz/BicycleGAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Realism (AMT Fooling Rate, Google maps→satellites): 34.33%",
    flowchart: "flowcharts/gifs/BiCycGAN.gif",
    highLevelIntuition: "Sample z, combine with A, generate output.\nEncode output back to latent; enforce cycle consistency for diversity and realism.",
    pros: [
      "High output realism and diversity",
      "Avoids mode collapse",
      "Faithful input-output mapping"
    ],
    cons: [
      "More complex training than baseline GANs",
      "Requires tuning of multiple loss terms",
      "Slightly higher computation"
    ],
    analogy: "A chef starts with a base dish and a surprise twist, creates a transformed meal, then asks another chef to taste and decode the twist—making sure every version is both a true reinterpretation and rich with unique, recoverable flavors, so no two plates ever feel the same.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1711.11586",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "ComboGAN",
    model: "Combo GAN",
    paperTitle: "ComboGAN: Unrestrained Scalability for Image Domain Translation",
    venue: "ECCV",
    year: 2018,
    exactDate: "2017-12-19",
    citations: 247,
    pdf: "https://arxiv.org/pdf/1712.06909",
    openSource: "https://github.com/AAnoosheh/ComboGAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "14 painters: 14 domains in 220 hours (vs 4 months for CycleGAN)",
    flowchart: "flowcharts/gifs/ComboGAN.gif",
    highLevelIntuition: "Encode input to a shared space. Decode to any target domain using the corresponding decoder.\n",
    pros: [
      "Linear scaling with number of domains",
      "Maintains image translation quality",
      "Flexible: new domains easily added"
    ],
    cons: [
      "Quality depends on shared latent space",
      "May hide details as imperceptible noise",
      "Requires uniform training over all domain pairs"
    ],
    analogy: "A chef learns the core essence of every world cuisine, then—when handed any local specialty—distills its unique base and instantly remakes it in the style of any other country’s kitchen, switching cultures with ease and serving up endless fusion dishes from a single pantry.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1712.06909",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PixSNAIL",
    model: "Pixel SNAIL",
    paperTitle: "PixelSNAIL: An Improved Autoregressive Generative Model",
    venue: "ICML",
    year: 2018,
    exactDate: "2017-12-28",
    citations: 351,
    pdf: "https://arxiv.org/pdf/1712.09763",
    openSource: "https://github.com/EugenHotaj/pytorch-generative/blob/master/pytorch_generative/models/autoregressive/pixel_snail.py",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10: 2.85 bits/dim",
    flowchart: "x₁,...,x_{i-1} → [causal conv] → [self-attention] → [mixture logistic head] → x_i\n",
    highLevelIntuition: "Causal convolutions gather information from nearby pixels.\nSelf-attention allows the model to focus on important pixels across the image for predicting the next one.",
    pros: [
      "State-of-the-art density estimation",
      "Combines strengths of convolutions and self-attention",
      "Models long-range pixel dependencies"
    ],
    cons: [
      "Slow sampling (sequential pixel generation)",
      "High computational cost",
      "Sampling speed similar to older autoregressive models"
    ],
    analogy: "A chef crafts a grand mosaic platter by carefully placing each new bite, using both a close-up taste of nearby flavors and a sweeping gaze across the whole table to decide every next move—so every piece harmonizes with both its neighbors and the banquet as a whole.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1712.09763",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "CharRNN",
    model: "Character RNN",
    paperTitle: "Character-level Recurrent Neural Networks in Practice",
    venue: "arXiv",
    year: 2018,
    exactDate: "2018-01-02",
    citations: 10,
    pdf: "https://arxiv.org/pdf/1801.00632",
    openSource: "https://github.com/karpathy/char-rnn",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "English dataset, 1 LSTM layer (512 units): Test perplexity ≈ 23",
    flowchart: "flowcharts/gifs/CharRNN.gif",
    highLevelIntuition: "Feed each token to update memory.\nPredict the next token from current memory.",
    pros: [
      "Flexible sequence modeling for any discrete token stream",
      "Robust to architecture, dataset, and parameter changes",
      "Supports efficient generation via progressive sampling"
    ],
    cons: [
      "Conditional multi-loss training (state transfer) can be unstable",
      "Training slower with single-loss approach",
      "Windowed sampling can be slow for long sequences"
    ],
    analogy: "A chef writes a story, adding one letter at a time, always peeking back at everything written so far to pick the next perfect letter—so each choice shapes and is shaped by the growing recipe on the page.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1801.00632",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "SNGAN",
    model: "Spectral Normalization GAN",
    paperTitle: "Spectral Normalization for Generative Adversarial Networks",
    venue: "ICLR",
    year: 2018,
    exactDate: "2018-02-16",
    citations: 6010,
    pdf: "https://arxiv.org/pdf/1802.05957",
    openSource: "https://github.com/godisboy/SN-GAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Inception Score (CIFAR-10, 2x updates): 8.69±0.09",
    flowchart: "z → [Generator G] → x_gen → [Discriminator D (SN applied)] → loss\n                ↑----------------------feedback----------------------↑\n",
    highLevelIntuition: "Add noise z and generate an image.\nDiscriminator with SN evaluates and trains both networks.\nSpectral normalization keeps D stable for better G learning.",
    pros: [
      "Stabilizes GAN training via Lipschitz constraint",
      "Computationally efficient (minimal overhead)",
      "Improves image diversity and robustness to hyperparameters"
    ],
    cons: [
      "Slightly slower than vanilla or weight normalization GANs",
      "May still underperform orthonormal regularization in rare cases",
      "Does not address all mode-collapse scenarios"
    ],
    analogy: "A chef invents new dishes from random flavors, but the food critic uses a special filter to make sure their taste scores stay within fair bounds—so every judging round is balanced, preventing wild overreactions and keeping the chef’s creativity on track.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1802.05957",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "AugCycGAN",
    model: "Augmented Cycle GAN",
    paperTitle: "Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data",
    venue: "ICML",
    year: 2018,
    exactDate: "2018-02-27",
    citations: 577,
    pdf: "https://arxiv.org/pdf/1802.10151",
    openSource: "https://github.com/aalmah/augmented_cyclegan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Edges-to-Photos (Shoes): Avg. L1 = 0.0698",
    flowchart: "flowcharts/gifs/AugCyc-GAN.gif",
    highLevelIntuition: "Combine input with random noise to map to the target domain.\nEncode extra information lost in mapping.\nMap back using encoded info to reconstruct the input.",
    pros: [
      "Learns many-to-many mappings from unpaired data",
      "Generates diverse, realistic outputs",
      "Robust to mode collapse and information hiding"
    ],
    cons: [
      "Slightly increased complexity over standard CycleGAN",
      "Requires tuning of auxiliary latent spaces",
      "Needs encoders for both domains"
    ],
    analogy: "A chef transforms a classic dish into many possible regional styles by adding surprise twists, carefully bottling up any secret flavors that get lost along the way, then uses those preserved notes to reverse the process and faithfully recreate the original plate—no matter how wild the culinary detour.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1802.10151",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "MUNIT",
    model: "Multimodal UNIT",
    paperTitle: "Multimodal Unsupervised Image-to-Image Translation (MUNIT)",
    venue: "ECCV",
    year: 2018,
    exactDate: "2018-04-12",
    citations: 3242,
    pdf: "https://arxiv.org/pdf/1804.04732",
    openSource: "https://github.com/nvlabs/MUNIT",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Edges→Shoes: Human Preference 50.0%, Diversity (LPIPS) 0.109",
    flowchart: "x₁ → [encode] → (c₁, s₁)\n          ↓\n   c₁ + s₂ (random)\n          ↓\n    [decode] → x₁→₂\n",
    highLevelIntuition: "Extract content and style codes from input. Combine content with random (or example) style code. Decode to generate a new, diverse image in the target domain.",
    pros: [
      "Generates diverse and multimodal outputs",
      "Does not require paired supervision",
      "Allows example-guided style control"
    ],
    cons: [
      "Needs separate encoders/decoders for each domain",
      "Diversity depends on style code sampling",
      "May require tuning for high-res images"
    ],
    analogy: "A chef splits a dish into its essential ingredients and its unique seasoning style, then mixes the core with a fresh, random set of spices to cook up endless new versions—so each serving keeps the original’s heart but bursts with surprising new flavors.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1804.04732",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PixDefend",
    model: "Pixel Defend",
    paperTitle: "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
    venue: "ICLR",
    year: 2018,
    exactDate: "2018-04-30",
    citations: 1030,
    pdf: "https://arxiv.org/pdf/1710.10766",
    openSource: "https://github.com/microsoft/PixelDefend",
    type: "Other",
    color: "#8b5cf6",
    primaryUseCase: "Image",
    bestPerformance: "Accuracy on CIFAR-10 strongest attack: 32% → 70%",
    flowchart: "X (input) → [PixelDefend purification] → X* (purified) → [Classifier] → y_pred",
    highLevelIntuition: "PixelDefend shifts an input image slightly to make it more likely under a generative model. The purified image is then sent to a classifier for prediction.",
    pros: [
      "Model-agnostic and attack-agnostic defense",
      "Significantly improves robustness to adversarial attacks",
      "Can be combined with other defenses for better results"
    ],
    cons: [
      "Slow due to autoregressive image generation",
      "Slightly reduces accuracy on clean images",
      "Exact constrained optimization is intractable"
    ],
    analogy: "A chef receives a suspicious dish and gently tweaks its seasoning and presentation, nudging it closer to familiar, trusted recipes, before serving it up for a final taste test—ensuring every plate is both safe and delicious, no matter what odd tricks may have slipped in.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1710.10766",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "SAGAN",
    model: "Self-Attention GAN",
    paperTitle: "Self-Attention Generative Adversarial Networks",
    venue: "ICML",
    year: 2019,
    exactDate: "2018-05-21",
    citations: 5365,
    pdf: "https://arxiv.org/pdf/1805.08318",
    openSource: "https://github.com/brain-research/self-attention-gan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Inception Score (ImageNet): 52.52",
    flowchart: "z, y → [Generator G with self-attention] → x_fake\nx_real, x_fake, y → [Discriminator D with self-attention] → real/fake\n",
    highLevelIntuition: "Sample noise and label; generate image using global attention.\nDiscriminator checks image realism and class consistency with attention across all regions.",
    pros: [
      "Models long-range dependencies via self-attention",
      "Improves image quality and consistency",
      "Stabilizes GAN training with spectral normalization"
    ],
    cons: [
      "Increased computational cost from attention",
      "More memory usage due to attention maps",
      "Effectiveness less pronounced for texture-only classes"
    ],
    analogy: "A chef crafts a grand banquet, not just focusing on each plate but scanning the whole table—carefully harmonizing distant flavors and decorations—so every dish, no matter where it’s placed, contributes perfectly to the feast’s overall balance and style.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1805.08318",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "GPT",
    model: "Generative Pre-Training",
    paperTitle: "Improving Language Understanding by Generative Pre-Training",
    venue: "OpenAI",
    year: 2018,
    exactDate: "2018-06-11",
    citations: 15130,
    pdf: "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf",
    openSource: "https://github.com/openai/finetune-transformer-lm",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "Story Cloze Test: 86.5% accuracy",
    flowchart: "U (text) → [Transformer LM] → h_n → [Fine-tuning] → Output (task label)",
    highLevelIntuition: "Train a Transformer to predict next words on large text.\nFine-tune on labeled data for each specific language task.",
    pros: [
      "Strong gains across many NLU tasks",
      "Requires minimal architectural changes for new tasks",
      "Captures long-range dependencies via Transformer"
    ],
    cons: [
      "Underperforms on very small datasets",
      "Only unidirectional context",
      "Limited zero-shot generalization compared to later models"
    ],
    analogy: "A chef reads cookbooks cover to cover, practicing by guessing each next ingredient in every recipe; when faced with a new cooking challenge, they quickly adapt old lessons, tweaking and plating dishes to suit any taste test that comes their way.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PixIQN",
    model: "Pixel IQN",
    paperTitle: "Autoregressive Quantile Networks for Generative Modeling",
    venue: "ICML",
    year: 2018,
    exactDate: "2018-06-14",
    citations: 97,
    pdf: "https://arxiv.org/pdf/1806.05575",
    openSource: "https://github.com/SSS135/aiqn-vae",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Image",
    bestPerformance: "ImageNet (32×32) Inception Score: 10.18 (class-conditional)",
    flowchart: "x₀ → [τ₁] → x₁ → [τ₂] → x₂ → ... → [τ_n] → x_n\n",
    highLevelIntuition: "Start with nothing; at each step, generate the next pixel using the previous pixels and a random value. Repeat for all pixels to build the full image.",
    pros: [
      "Higher perceptual quality and global consistency",
      "Maintains high sample diversity, avoids mode collapse",
      "Stable and robust optimization, insensitive to hyperparameters"
    ],
    cons: [
      "Sampling is slow due to autoregressive nature",
      "Does not explicitly model likelihood, limiting density queries",
      "Not compatible with PixelCNN++ mixture-of-logistics head"
    ],
    analogy: "A chef builds a dish from scratch, adding each new ingredient one by one, but for every addition, rolls a special flavor die to guide the next step—ensuring the final meal is not only well-balanced and coherent, but always full of delightful, unpredictable variety.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1806.05575",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "pix2pixHD",
    model: "pix2pixHD",
    paperTitle: "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
    venue: "CVPR",
    year: 2018,
    exactDate: "2018-06-18",
    citations: 5593,
    pdf: "https://arxiv.org/pdf/1711.11585",
    openSource: "https://github.com/NVIDIA/pix2pixHD",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "Cityscapes Pixel Accuracy: 83.78%",
    flowchart: "s, boundary, features → [G₁: global gen] → [G₂: local enhancer] → x_fake\n                         ↑                                   |\n                 E(x): instance encoder  ←──────────────┘\n",
    highLevelIntuition: "First, extract semantic and instance structure from the label map.\nGenerate a globally coherent low-res image, then refine to full resolution using local details and features.",
    pros: [
      "Synthesizes 2048×1024 photo-realistic images from semantic maps",
      "Multi-scale generator and discriminator enable high-res, detailed results",
      "Supports interactive and diverse object-level editing"
    ],
    cons: [
      "High computational and memory requirements",
      "Relies on accurate instance/semantic maps for best results",
      "May still miss some ultra-fine details without perceptual loss"
    ],
    analogy: "A chef starts by outlining a grand buffet’s layout on a map, prepares a full spread of miniature sample plates to ensure every section fits the theme, then zooms in to garnish each dish with local spices and decorative touches—transforming a rough plan into a high-resolution culinary masterpiece.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1711.11585",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "CNF",
    model: "Continuous Normalized Flows",
    paperTitle: "Neural Ordinary Differential Equations",
    venue: "NeurIPS",
    year: 2018,
    exactDate: "2018-06-19",
    citations: 7146,
    pdf: "https://arxiv.org/pdf/1806.07366",
    openSource: "https://github.com/rtqichen/torchdiffeq",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "MNIST Test Error: 0.42% (ODE-Net, Table 1)",
    flowchart: "flowcharts/gifs/CNF.gif",
    highLevelIntuition: "Transform latent variables using neural ODEs to map noise to data.\nCompute probability by tracking density change along the trajectory.",
    pros: [
      "Constant memory cost as a function of depth",
      "Invertible and scalable for normalizing flows",
      "Adaptive computation (speed–accuracy tradeoff)"
    ],
    cons: [
      "Mini-batching is less straightforward",
      "Requires setting error tolerances for training/inference",
      "Backward trajectory reconstruction may add numerical error"
    ],
    analogy: "A chef gently stirs a simmering stew, continuously adjusting heat and seasoning in a smooth, unbroken flow—watching the evolving aroma to ensure the flavor transformation is precise at every instant, from bland ingredients to a perfectly balanced masterpiece.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1806.07366",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "Glow",
    model: "Glow",
    paperTitle: "Glow: Generative Flow with Invertible 1x1 Convolutions",
    venue: "NeurIPS",
    year: 2018,
    exactDate: "2018-07-09",
    citations: 3961,
    pdf: "https://arxiv.org/pdf/1807.03039",
    openSource: "https://github.com/openai/glow",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 bits/dim: 3.35",
    flowchart: "x₀ → [actnorm] → h₁ → [invertible 1×1 conv] → h₂ → [affine coupling] → x₁",
    highLevelIntuition: "Actnorm normalizes channels.\nInvertible 1×1 conv mixes channels.\nAffine coupling transforms half the data using the other half.\nRepeat to map data to a simple latent distribution.",
    pros: [
      "Exact latent-variable inference and log-likelihood evaluation",
      "Highly parallelizable inference and synthesis",
      "Enables smooth latent-space interpolations and meaningful data manipulation"
    ],
    cons: [
      "Slightly higher computational cost vs. fixed permutations",
      "Lower sample quality than GANs for very high-res images",
      "Sensitive to choice of architecture and hyperparameters"
    ],
    analogy: "A chef layers a salad by first balancing every flavor, then tossing the mix with a precise spinning bowl that intermingles ingredients, and finally pairs halves of the bowl to dress each other—repeating this series until the jumble of vegetables transforms into a dish with perfectly blended, easily reversible flavors.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1807.03039",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "BigGAN",
    model: "Big GAN",
    paperTitle: "Large Scale GAN Training for High Fidelity Natural Image Synthesis (BigGAN)",
    venue: "ICLR",
    year: 2019,
    exactDate: "2018-09-28",
    citations: 7052,
    pdf: "https://arxiv.org/pdf/1809.11096",
    openSource: "https://github.com/lukemelas/pytorch-pretrained-gans",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "ImageNet 128×128: Inception Score (IS) 166.5",
    flowchart: "flowcharts/gifs/BigGAN.gif",
    highLevelIntuition: "Noise and class label are fed to the generator to synthesize images.\nDiscriminator learns to tell real from generated images, guiding the generator.",
    pros: [
      "State-of-the-art image fidelity and diversity on large-scale datasets",
      "Scalable to high resolutions (up to 512×512)",
      "Truncation trick enables fine-grained trade-off between fidelity and variety"
    ],
    cons: [
      "Requires massive computational resources",
      "Training instability at large scale (risk of collapse)",
      "Discriminator overfits without careful regularization"
    ],
    analogy: "A chef with an army-sized pantry and limitless kitchen whips up grand banquets from scratch, using both random inspiration and strict menu themes, while a panel of gourmet judges constantly samples and compares every plate—pushing each creation toward both dazzling realism and boundless variety.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1809.11096",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "FFJORD",
    model: "FFJORD",
    paperTitle: "FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models",
    venue: "ICLR",
    year: 2019,
    exactDate: "2018-10-02",
    citations: 1055,
    pdf: "https://arxiv.org/pdf/1810.01367",
    openSource: "https://github.com/rtqichen/ffjord",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "MNIST bits/dim: 0.99",
    flowchart: "z₀ → [integrate f(z, t)] → zₜ, log p(zₜ)\n    ↓                  ↑\n  Hutchinson Estimator |\n    ↓                  ↑\n  log p(z₀)      ←  ∫ₜ₀^ₜ₁ Tr(∂f/∂z) dt\n",
    highLevelIntuition: "Sample a base point. Integrate a neural ODE to warp it into data space, updating the log-probability using the trace estimator.",
    pros: [
      "Unrestricted, flexible neural architectures",
      "Exact log-likelihood and one-pass sampling",
      "Models discontinuous and multi-modal densities"
    ],
    cons: [
      "High computational cost due to many ODE solves",
      "Slower training than competitors",
      "Sensitive to ODE solver and numerical errors"
    ],
    analogy: "A chef takes a simple base broth and, through a continuous, free-form simmer with swirling stirs and timed infusions, evolves it into a complex stew—tracking every subtle shift in flavor along the way to ensure the finished dish is as rich and layered as possible.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1810.01367",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "CVAE",
    model: "Conditional Variational Autoencoder for Neural Machine Translation",
    paperTitle: "Conditional Variational Autoencoder for Neural Machine Translation",
    venue: "arXiv",
    year: 2018,
    exactDate: "2018-12-11",
    citations: 78,
    pdf: "https://arxiv.org/pdf/1812.04405",
    openSource: "https://github.com/timbmg/VAE-CVAE-MNIST",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Image",
    bestPerformance: "BLEU (Greedy): 31.2 on IWSLT 2016 De→En",
    flowchart: "flowcharts/gifs/CVAE.gif",
    highLevelIntuition: "Encode x for global context.\nInfer z from both x and y using co-attention.\nDecode y using both x and z for richer translations.",
    pros: [
      "Captures semantic and stylistic translation variations",
      "Outperforms seq2seq and previous variational baselines",
      "Co-attention inference improves latent expressiveness"
    ],
    cons: [
      "Suffers from posterior collapse without careful training",
      "KL divergence tuning required for stable learning",
      "Performance gains are dataset and setting-dependent"
    ],
    analogy: "A chef tastes a dish and notes the main flavors, then, while preparing a translation for a foreign menu, samples both the source and target cuisines—drawing on this dual tasting to craft a new plate that captures not just the meaning, but the subtle style and flair of both tables.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1812.04405",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "MADGAN",
    model: "Multivariate Anomaly Detection GAN",
    paperTitle: "MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks",
    venue: "ICDM",
    year: 2019,
    exactDate: "2019-01-15",
    citations: 1420,
    pdf: "https://arxiv.org/pdf/1901.04997",
    openSource: "https://github.com/LiDan456/MAD-GANs",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Tabular",
    bestPerformance: "SWaT Recall: 99.98%",
    flowchart: "z → [G] → x_fake\nx_real, x_fake → [D] → real/fake score\n\nFor test:\nx_test → [latent inversion] → z*\nz* → [G] → x_recon\nx_test, x_recon → [Res] → residual\nx_test → [D] → discrimination\n[Residual + Discrimination] → DR-Score → Anomaly\n",
    highLevelIntuition: "Generator creates sequences from noise to mimic real data.\nDiscriminator tries to distinguish real from fake sequences.\nDuring detection, both reconstruction error and discriminator score are combined as anomaly signal.",
    pros: [
      "Captures complex temporal and multivariate correlations",
      "Uses both generator and discriminator for detection",
      "Achieves near-perfect recall in cyber-attack detection"
    ],
    cons: [
      "High false positive rate on unbalanced datasets",
      "Slower training with longer sub-sequences",
      "Instability across training epochs"
    ],
    analogy: "A chef invents new dishes to match the daily menu, while a sharp-tongued critic tastes each plate to spot any odd flavors; if something seems off, the chef tries to recreate the suspicious dish and compares results—using both their reconstruction skills and the critic’s judgment to flag every anomaly on the menu.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1901.04997",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "Flow++",
    model: "Flow++",
    paperTitle: "Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design",
    venue: "ICML",
    year: 2019,
    exactDate: "2019-02-01",
    citations: 562,
    pdf: "https://arxiv.org/pdf/1902.00275",
    openSource: "https://github.com/aravindsrinivas/flowpp",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR10 bits/dim: 3.08",
    flowchart: "x₀ → [add dequant noise u] → y → [flow f] → z\n     ↑                   ↓\n   [q(u|x)]         [f = stack of invertible flows]\n",
    highLevelIntuition: "First, noise is cleverly added to make pixel values continuous.\nThen, a deep invertible network transforms the noisy data into Gaussian noise for density modeling.",
    pros: [
      "State-of-the-art non-autoregressive density estimation",
      "Fast and efficient sampling",
      "Matches perceptual quality of PixelCNN"
    ],
    cons: [
      "Still lags behind top autoregressive models",
      "More complex architecture than earlier flows",
      "Slightly higher bits/dim than PixelCNN++"
    ],
    analogy: "A chef sprinkles a dash of playful chaos into each bite of a dish, then runs the meal through a series of intricate blending steps—transforming the food until it tastes as pure and simple as possible, ready for analysis or serving with effortless flair.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1902.00275",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "GPT2",
    model: "GPT-2",
    paperTitle: "Language Models are Unsupervised Multitask Learners",
    venue: "OpenAI",
    year: 2019,
    exactDate: "2019-02-15",
    citations: 17612,
    pdf: "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
    openSource: "https://github.com/openai/gpt-2",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "LAMBADA Perplexity: 8.6 (State of the art on 7/8 LM datasets)",
    flowchart: "x₀ → [transformer] → p(x₁|x₀) → [sample x₁] → x₁ → [transformer] → p(x₂|x₀,x₁) → ...\n",
    highLevelIntuition: "Feed prompt into the transformer to get token probabilities.\nPredict the next token based on the context.\nAppend token, repeat prediction for next position.",
    pros: [
      "Zero-shot performance on diverse NLP tasks",
      "State-of-the-art on most language modeling datasets",
      "Requires no task-specific supervision or architecture change"
    ],
    cons: [
      "Still underfits training data (WebText)",
      "Poor on tasks needing structured outputs (e.g., summarization)",
      "Limited by unidirectional context (no bidirectional info)"
    ],
    analogy: "A chef listens to a few lines of a story and, without any recipes, improvises the next ingredient or step—building the dish one move at a time—always basing each new addition on everything tasted so far, and never needing to see the whole recipe in advance.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "δVAE",
    model: "δ-VAE",
    paperTitle: "Preventing Posterior Collapse with delta-VAEs",
    venue: "ICLR",
    year: 2019,
    exactDate: "2019-05-09",
    citations: 227,
    pdf: "http://arxiv.org/pdf/1901.03416",
    openSource: "https://www.catalyzex.com/paper/preventing-posterior-collapse-with-delta-vaes/code",
    type: "VAE",
    color: "#eab308",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 NLL: ≤ 2.83 bits/dim",
    flowchart: "flowcharts/gifs/DeltaVAE.gif",
    highLevelIntuition: "Encode input to a latent z while ensuring the encoding differs from the prior by at least δ.\nDecode z to reconstruct input, using a strong autoregressive prior to model sequential structure.",
    pros: [
      "Prevents posterior collapse without altering ELBO",
      "Learns useful representations with powerful decoders",
      "Achieves state-of-the-art likelihoods on images"
    ],
    cons: [
      "Requires careful choice of prior-posterior families",
      "Performance sensitive to mismatch between prior and aggregate posterior",
      "Auxiliary prior needed for best results"
    ],
    analogy: "A chef insists that every new recipe must use at least a dash of a special spice, ensuring each dish stands apart from the plain pantry base—then cooks up the meal, layering in rich, sequential flavors that always preserve a minimum taste difference from the original stock.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1901.03416",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "StyleGAN",
    model: "Style GAN",
    paperTitle: "A Style-Based Generator Architecture for Generative Adversarial Networks",
    venue: "CVPR",
    year: 2019,
    exactDate: "2019-06-16",
    citations: 14994,
    pdf: "https://arxiv.org/pdf/1812.04948",
    openSource: "https://github.com/NVlabs/stylegan",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "FFHQ FID: 4.40",
    flowchart: "z → [Mapping f] → w → [Affine] → y → [AdaIN+Noise+Conv]* → x_out\n",
    highLevelIntuition: "Random noise z is mapped to an intermediate latent w for disentanglement.\nAt each scale, style y and random noise control different features, creating images in a coarse-to-fine manner.",
    pros: [
      "Unsupervised disentanglement of high-level and stochastic features",
      "Intuitive, scale-specific control over generated images",
      "State-of-the-art image quality and interpolation"
    ],
    cons: [
      "Increased model complexity and parameter count",
      "Training instability with deep mapping networks at high learning rates",
      "Requires significant computational resources for high-resolution results"
    ],
    analogy: "A chef first invents a secret master recipe, then customizes every stage of a cake’s creation—from broad shape to fine sprinkles—using separate dials for big and small details, blending unique flair at each layer until the dessert is both artful and deliciously original.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1812.04948",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "SinGAN",
    model: "Single GAN",
    paperTitle: "SinGAN: Learning a Generative Model from a Single Natural Image",
    venue: "ICCV",
    year: 2019,
    exactDate: "2019-08-17",
    citations: 1128,
    pdf: "https://arxiv.org/pdf/1905.01164",
    openSource: "https://github.com/tamarott/SinGAN",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "AMT “Real/Fake” Unpaired Confusion Rate: 47.04% (N–1 scale, Table 1)",
    flowchart: "z_N → [G_N] → x̃_N\n   ↓           ↑\nz_{N-1} → [G_{N-1}] → x̃_{N-1}\n   ↓           ↑\n ...      ...\n   ↓           ↑\nz_0 → [G_0] → x̃_0\n",
    highLevelIntuition: "Start with random noise at the coarsest scale to generate a rough layout.\nEach finer scale adds details, using noise and the previous upsampled result, until the final image.",
    pros: [
      "Learns from a single natural image—no dataset required",
      "Generates diverse, high-quality samples preserving structure and texture",
      "Flexible for multiple image manipulation tasks (super-resolution, harmonization, editing, animation)"
    ],
    cons: [
      "Semantic diversity limited to training image content",
      "Performance depends on image complexity and chosen scales",
      "Not designed for generating new semantic classes"
    ],
    analogy: "A chef recreates a single favorite dish from memory, first sculpting a rough outline from broad strokes, then layering on more and more intricate textures with each pass—using fresh improvisations at every step until the final plate both echoes and expands on the original’s flavor.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1905.01164",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "IResNetFlows",
    model: "I-ResNet Flows",
    paperTitle: "I-ResNet Flows: On Invertible Residual Networks",
    venue: "ICLR",
    year: 2020,
    exactDate: "2019-09-20",
    citations: 776,
    pdf: "https://arxiv.org/pdf/1811.00995",
    openSource: "https://github.com/jhjacobsen/invertible-resnet",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR10 bits/dim: 3.45",
    flowchart: "z ~ N(0, I) → [invertible ResNet F^{-1}] → x\nx → [F] → z,  logdet: tr(ln(I + J_g(x)))\n",
    highLevelIntuition: "Sample z from a simple distribution.\nInvert through stacked residual blocks to generate x.",
    pros: [
      "Unified architecture for classification and generative modeling",
      "Free-form (no dimension-splitting) residual blocks",
      "Competitive with state-of-the-art discriminative and flow-based models"
    ],
    cons: [
      "Log-determinant estimator is biased",
      "Slightly slower generative modeling vs. non-invertible flows",
      "Requires careful spectral normalization for each block"
    ],
    analogy: "A chef starts with pure, basic flavors and passes them through a stack of creative, perfectly reversible recipes—each transformation bold yet always undoable—so the final meal is both adventurous and able to be traced exactly back to its roots.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1811.00995",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "MelGAN",
    model: "Mel-spectrogram GAN",
    paperTitle: "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis",
    venue: "NeurIPS",
    year: 2019,
    exactDate: "2019-10-08",
    citations: 1289,
    pdf: "https://arxiv.org/pdf/1910.06711",
    openSource: "https://github.com/descriptinc/melgan-neurips",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Audio",
    bestPerformance: "MOS (LJ Speech): 3.61 ± 0.06",
    flowchart: "s (mel-spectrogram)\n    ↓\n[G: conv + upsampling + residual blocks]\n    ↓\nx (waveform)\n    ↓\n[D₁, D₂, D₃: multi-scale window discrim.]\n    ↓\n{real/fake, features}\n",
    highLevelIntuition: "Mel-spectrogram is upsampled to audio by generator.\nDiscriminators judge audio realism at multiple resolutions.",
    pros: [
      "Non-autoregressive, fully convolutional, fast inference",
      "Very lightweight: only 4.26M parameters",
      "Generalizes to unseen speakers for mel-spectrogram inversion"
    ],
    cons: [
      "Slightly lower quality than state-of-the-art autoregressive models",
      "Requires time-aligned conditioning (mel-spectrogram)",
      "Needs paired data for feature matching loss"
    ],
    analogy: "A chef receives a sheet of musical notes describing a melody, then uses a fast series of mixing and layering steps to turn those notes into a full, vibrant meal—while several food critics taste each dish at different levels of detail to make sure every version feels genuinely delicious.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1910.06711",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "MintNet",
    model: "Mint Net",
    paperTitle: "MintNet: Building Invertible Neural Networks with Masked Convolutions",
    venue: "NeurIPS",
    year: 2019,
    exactDate: "2019-10-28",
    citations: 87,
    pdf: "https://arxiv.org/pdf/1907.07945",
    openSource: "https://github.com/ermongroup/MintNet",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "MNIST bits per dimension: 0.98",
    flowchart: "x₀ → [masked convs + activations] → x_t → [fixed-point inverse] → x₀\n",
    highLevelIntuition: "Apply a stack of masked convolutions and nonlinearities to encode the image.\nEfficiently invert the network using parallel fixed-point iterations for sampling or reconstruction.",
    pros: [
      "Exact and efficient Jacobian determinant computation",
      "Inversion is fast, parallelizable, and efficient",
      "Fewer parameters and faster sampling than Glow, FFJORD"
    ],
    cons: [
      "Slightly lower classification accuracy than non-invertible ResNet",
      "Inversion relies on fixed-point iteration (may need tuning)",
      "Still slower than some models for analytic inverse"
    ],
    analogy: "A chef transforms a salad through a secret sequence of intricate slicing and tossing moves, making the dish more complex with each pass—yet, thanks to clever techniques, can always reverse the process step by step and perfectly recreate the original bowl on demand.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1907.07945",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "BART",
    model: "BART",
    paperTitle: "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    venue: "ACL",
    year: 2020,
    exactDate: "2019-10-29",
    citations: 13643,
    pdf: "https://arxiv.org/pdf/1910.13461",
    openSource: "https://github.com/facebookresearch/fairseq",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "XSum ROUGE-1: 45.14",
    flowchart: "flowcharts/gifs/BART.gif",
    highLevelIntuition: "Corrupt the original text with noise. Encode the noisy text, then autoregressively decode to reconstruct the original.",
    pros: [
      "Strong performance on both text generation and comprehension tasks",
      "Flexible noising schemes enable robust pretraining",
      "Generalizes both BERT and GPT architectures"
    ],
    cons: [
      "Less effective on tasks where output is loosely constrained by input (e.g., ELI5)",
      "Larger model size compared to BERT",
      "Dependent on large-scale pretraining data"
    ],
    analogy: "A chef deliberately messes up a recipe—scrambling steps, hiding ingredients, or spilling sauces—then practices fixing and restoring the meal, so with enough repetition, they become experts at turning chaos back into a flawless, delicious dish.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1910.13461",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "ScoreBasedSDE",
    model: "Score-Based Generative Modeling through Stochastic Differential Equations",
    paperTitle: "Score-Based Generative Modeling through Stochastic Differential Equations",
    venue: "ICLR",
    year: 2020,
    exactDate: "2019-11-03",
    citations: 8177,
    pdf: "https://arxiv.org/pdf/2011.13456",
    openSource: "https://github.com/yang-song/score_sde_pytorch",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 Unconditional: FID 2.20, IS 9.89",
    flowchart: "x₀ → [forward SDE: add noise] → x_T\nx_T → [reverse SDE: use ∇ₓ log p_t(x)] → x₀",
    highLevelIntuition: "Data is gradually noised until it becomes random. The process is then reversed using a learned function to convert noise back into data.",
    pros: [
      "Unified framework covering DDPM/SMLD as SDE special cases",
      "Enables both exact likelihood computation and flexible sampling",
      "Achieves state-of-the-art image generation quality"
    ],
    cons: [
      "Sampling is slower than GANs",
      "Many sampler hyperparameters to tune",
      "Performance depends on score network accuracy"
    ],
    analogy: "A chef slowly muddles a perfect soup by adding pinch after pinch of random spices until it’s unrecognizable, then carefully retraces their steps—guided by a refined palate—to remove the chaos and restore the original, savory masterpiece.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2011.13456",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "BigBiGAN",
    model: "Big Bidirectional GAN",
    paperTitle: "Large Scale Adversarial Representation Learning",
    venue: "NeurIPS",
    year: 2019,
    exactDate: "2019-12-08",
    citations: 653,
    pdf: "https://arxiv.org/pdf/1907.02544",
    openSource: "https://github.com/lukemelas/pytorch-pretrained-gans",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "ImageNet Top‑1 (unsupervised): 61.3%",
    flowchart: "flowcharts/gifs/BigBiGAN.gif",
    highLevelIntuition: "Sample z, generate image x̂; sample x, encode to ẑ.\nDiscriminator distinguishes (real x, encoded ẑ) vs (generated x̂, real z).\nEncoder and generator are trained to fool discriminator; their features capture data semantics.",
    pros: [
      "State-of-the-art unsupervised representation learning on ImageNet",
      "Strong unconditional image generation (high IS, low FID)",
      "Scalable to high resolutions and large architectures"
    ],
    cons: [
      "Training is computationally expensive",
      "Requires careful architecture and optimization tuning",
      "Reconstructions lack pixel-level accuracy, focusing on semantics"
    ],
    analogy: "A chef creates dishes from secret ingredient lists, while another tries to guess the ingredients of real meals; a sharp-eyed critic tastes pairs of dishes and recipes—spotting mismatches between homemade plates and their supposed ingredient lists—pushing both chefs to perfect the art of matching flavors with secrets.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1907.02544",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "NCSN",
    model: "Noise Conditional Score Network",
    paperTitle: "Generative Modeling by Estimating Gradients of the Data Distribution",
    venue: "NeurIPS",
    year: 2019,
    exactDate: "2019-12-14",
    citations: 4923,
    pdf: "https://arxiv.org/pdf/1907.05600",
    openSource: "https://github.com/ermongroup/ncsn",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 Inception Score: 8.87",
    flowchart: "x₀ → [add σ₁ noise] → x₁\nx₁ → [Langevin step: predict sθ(x₁, σ₁)] → x₂ → ... → x_T\n↓\n[decrease σ to σ₂]\nRepeat: x_T → [predict sθ(x_T, σ₂)] → ... → x_T'\n...\n[repeat until σ_L ≈ 0]\n",
    highLevelIntuition: "Start from random noise; denoise stepwise using neural network gradients.\nGradually reduce noise level, refining the sample closer to real data at each stage.",
    pros: [
      "Generates high-quality, diverse images",
      "No adversarial or likelihood-based training required",
      "Flexible architecture; tractable objective for deep networks"
    ],
    cons: [
      "Sampling is slow due to iterative Langevin dynamics",
      "Sensitive to choice of noise schedule and step size",
      "FID not SOTA; can lag GANs in some metrics"
    ],
    analogy: "A chef starts with a bowl of chaotic, noisy spices and slowly refines the blend—tasting and adjusting bit by bit—steadily reducing randomness until the seasoning becomes a perfectly balanced, flavorful mix.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1907.05600",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "NSF",
    model: "Neural Spline Flows",
    paperTitle: "Neural Spline Flows",
    venue: "NeurIPS",
    year: 2019,
    exactDate: "2019-12-14",
    citations: 1013,
    pdf: "https://arxiv.org/pdf/1906.04032",
    openSource: "https://github.com/bayesiains/nsf",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "ImageNet64 8-bit BPD: 3.82",
    flowchart: "u → [f] → x\nx → [f⁻¹, spline] → u\n",
    highLevelIntuition: "Sample noise, pass through invertible spline network to get data.\nTo compute likelihood, invert data through spline back to noise.",
    pros: [
      "One-pass exact density evaluation and sampling",
      "Significantly more flexible than affine flows",
      "Competitive or superior performance with fewer parameters"
    ],
    cons: [
      "More complex implementation than affine flows",
      "Slightly increased computation per step (30–40% slower)",
      "Marginal improvement over affine flows in some VAEs"
    ],
    analogy: "A chef starts with a simple dough and shapes it through a series of precise, bendable folds—like smooth, flowing ribbons—so the final pastry is both flexible and intricate, yet can be unfolded perfectly back into its original form.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1906.04032",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "VQVAE2",
    model: "VQ-VAE 2",
    paperTitle: "Generating Diverse High-Fidelity Images with VQ-VAE-2",
    venue: "NeurIPS",
    year: 2019,
    exactDate: "2019-12-14",
    citations: 2400,
    pdf: "https://arxiv.org/pdf/1906.00446.pdf",
    openSource: "https://github.com/rosinality/vq-vae-2-pytorch",
    type: "VAE",
    color: "#eab308",
    primaryUseCase: "Image",
    bestPerformance: "ImageNet-256 Top‑1 CAS: 54.83%",
    flowchart: "x → Encoder → Quantize → [e_top, e_bottom] \n       ↓                       ↓\n    Decoder ← [PixelCNN prior over e_top & e_bottom] \n       ↓\n    x̂ (generated or reconstructed image)\n",
    highLevelIntuition: "Compress image to multi-level codes.\nSample codes with PixelCNN, decode to image.",
    pros: [
      "Generates diverse, high-fidelity images",
      "Faster sampling than pixel-space models",
      "Competitive with state-of-the-art GANs"
    ],
    cons: [
      "Slightly lower image sharpness than GANs",
      "Large memory use for high-res priors",
      "Some blurriness in reconstructions"
    ],
    analogy: "A chef first breaks down a complex recipe into broad base sauces and fine garnishes, stores these separately, then uses a skilled sous-chef to mix and match combinations—layering from coarse to fine—to recreate vibrant, detailed dishes faster than starting from scratch each time.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1906.00446",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "T5",
    model: "Text-to-Text Transfer Transformer",
    paperTitle: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    venue: "JMLR",
    year: 2020,
    exactDate: "2020-04-01",
    citations: 25531,
    pdf: "https://arxiv.org/pdf/1910.10683.pdf",
    openSource: "https://github.com/google-research/text-to-text-transfer-transformer",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "GLUE average: 83.28",
    flowchart: "x₀ (input text) \n  → [random span masking]\n  → x̃ (masked input with sentinels)\n  → [encoder]\n  → z\n  → [decoder generates target sequence]\n  → y (masked spans in order)\n",
    highLevelIntuition: "Randomly mask text spans in input. Encode masked input, then decode the dropped spans in order.",
    pros: [
      "Unified text-to-text framework for all NLP tasks",
      "State-of-the-art results on diverse benchmarks",
      "Highly scalable to large model/data sizes"
    ],
    cons: [
      "High computational and memory requirements",
      "Needs extensive data cleaning for pretraining",
      "Model size can limit deployment"
    ],
    analogy: "A chef covers parts of a recipe with opaque lids, studies the partially hidden instructions, then carefully recreates the missing steps in the correct order—mastering the full dish by filling in gaps from context and taste.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1910.10683",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "GPT3",
    model: "GPT-3",
    paperTitle: "Language Models are Few-Shot Learners",
    venue: "NeurIPS",
    year: 2020,
    exactDate: "2020-05-28",
    citations: 51206,
    pdf: "https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf",
    openSource: "https://github.com/openai/gpt-3",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "Few-shot LAMBADA accuracy: 86.4%",
    flowchart: "x₀ → [context prompt] → x₁, x₂, ..., x_t → [autoregressive LM] → x_{t+1}\n",
    highLevelIntuition: "Model reads prompt and previous tokens. It generates next token, repeating step until output is complete.",
    pros: [
      "Strong few-shot and zero-shot performance without fine-tuning",
      "Scales well with model size",
      "Matches or surpasses prior SOTA on several NLP benchmarks"
    ],
    cons: [
      "Large compute and memory requirements",
      "Weaknesses on tasks needing bidirectional context or sentence comparison",
      "Prone to bias, repetition, and incoherence on long passages"
    ],
    analogy: "A master chef reads a few sample dishes and their descriptions, then improvises new recipes step-by-step—each addition inspired by everything tasted so far—creating elaborate meals without ever needing full instructions upfront.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2005.14165",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "NVAE",
    model: "Hierarchical VAE",
    paperTitle: "NVAE: A Deep Hierarchical Variational Autoencoder",
    venue: "NeurIPS",
    year: 2020,
    exactDate: "2020-06-01",
    citations: 1224,
    pdf: "https://arxiv.org/pdf/2007.03898",
    openSource: "https://github.com/NVlabs/NVAE",
    type: "VAE",
    color: "#eab308",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10: 2.91 bits/dim",
    flowchart: "x₀ → [encode] → {z₁, z₂, ..., z_L}\nz₁ → [sample] → z₂ → ... → z_L\n{z₁,...,z_L} → [decode] → x̂\n",
    highLevelIntuition: "Encode the input image into multiple groups of latent variables, capturing structure at different scales. Sample each group step-by-step, then decode all latents to reconstruct or generate an image.",
    pros: [
      "State-of-the-art for non-autoregressive likelihood models on multiple datasets",
      "Fast, tractable sampling and scalable to 256×256 images",
      "Stabilized training via spectral regularization and residual posteriors"
    ],
    cons: [
      "Slightly behind best autoregressive models on some datasets",
      "High GPU memory usage for large images",
      "Limited expressivity compared to full autoregressive decoders"
    ],
    analogy: "A chef breaks down a complex recipe into multiple layers—starting with broad base sauces, then mid-level seasonings, and finally delicate garnishes—preparing and tasting each layer in sequence before combining them all into a perfectly balanced final dish.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2007.03898",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "GaussFlows",
    model: "Gaussianization Flows",
    paperTitle: "Gaussianization Flows",
    venue: "AISTATS",
    year: 2020,
    exactDate: "2020-06-05",
    citations: 44,
    pdf: "https://arxiv.org/pdf/2003.01941",
    openSource: "https://github.com/chenlin9/Gaussianization_Flows",
    type: "Flow",
    color: "#0ea5e9",
    primaryUseCase: "Tabular",
    bestPerformance: "Top NLL on 3/5 UCI tabular datasets; robust on small data, e.g., GAS: −10.13 nats (best among compared flows).",
    flowchart: "x → [rotation R₁] → x₁ → [kernel layer Ψ₁] → x₂ → [rotation R₂] → ... → [kernel layer Ψ_L] → z\n",
    highLevelIntuition: "Rotate data to mix dependencies.\nGaussianize each dimension with kernel layers.\nAlternate these steps to gradually “whiten” the distribution.\nFinal output matches standard Gaussian.",
    pros: [
      "Universal approximator with efficient inversion and likelihood.",
      "Robust to distribution shifts and data reparameterization.",
      "Superior generalization with small datasets."
    ],
    cons: [
      "May be less scalable for very high-dimensional data (rotation cost).",
      "Patch-based rotations may trade off some expressivity for efficiency.",
      "Performance may depend on kernel layer hyperparameters."
    ],
    analogy: "A chef repeatedly spins and stirs a complex stew, then adds secret seasonings that smooth out each flavor dimension—alternating these steps to gradually mellow and blend the ingredients until the final broth tastes like a perfectly balanced, classic consommé.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2003.01941",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DDPM",
    model: "Denoising Diffusion Probabilistic Models",
    paperTitle: "Denoising Diffusion Probabilistic Models",
    venue: "NeurIPS",
    year: 2020,
    exactDate: "2020-06-11",
    citations: 25194,
    pdf: "https://arxiv.org/pdf/2006.11239",
    openSource: "https://github.com/hojonathanho/diffusion",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR10 FID: 3.17",
    flowchart: "flowcharts/gifs/DDPM.gif",
    highLevelIntuition: "Noise is gradually added to the image in many small steps.\nThe model then learns to reverse this process by denoising step-by-step.",
    pros: [
      "State-of-the-art sample quality for unconditional image generation",
      "Stable and easy to train",
      "Strong inductive bias for lossy image compression"
    ],
    cons: [
      "Slow sampling (requires many steps)",
      "Log likelihood worse than some likelihood-based models",
      "Not ideal for fast real-time generation"
    ],
    analogy: "A chef slowly sprinkles salt and spices onto a fresh salad until it’s completely overwhelmed, then carefully tastes and removes excess seasoning bit by bit—restoring the original crisp, fresh flavors one step at a time.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2006.11239",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "FCE",
    model: "Flow Contrastive Estimation",
    paperTitle: "Flow Contrastive Estimation for Training Normalizing Flows",
    venue: "NeurIPS",
    year: 2020,
    exactDate: "2020-06-24",
    citations: 128,
    pdf: "https://arxiv.org/pdf/1912.00589",
    openSource: "https://github.com/volagold/fce-2d",
    type: "Flow + EBM",
    color: "#0ea5e9",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 FID: 37.30",
    flowchart: "z ~ q₀(z) → [g_α] → x̃ ~ q_α(x)\nx (real or synthetic) → [f_θ(x)] → compute p_θ(x)\nx, x̃ → [logistic regression] → update θ, α via V(θ, α)\n",
    highLevelIntuition: "Sample noise, transform it via the flow to generate fake data.\nEvaluate both real and fake samples with the EBM.\nContrast EBM and flow to improve both using an adversarial game.",
    pros: [
      "Jointly trains EBM and flow models for improved sample quality",
      "Avoids MCMC in EBM training (uses contrastive estimation)",
      "Enables strong unsupervised feature learning and adapts to semi-supervised tasks"
    ],
    cons: [
      "Requires iterative adversarial updates between two models",
      "Slightly lower log-likelihood than MLE-trained Glow",
      "Still relies on flow model expressiveness as contrast"
    ],
    analogy: "A chef experiments by crafting new dishes from random ingredients while a savvy taster compares these creations against classic recipes—each round, both refine their skills through a playful duel, pushing the chef to invent better flavors and the taster to sharpen their discerning palate.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1912.00589",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "StyleGAN2",
    model: "Style GAN 2",
    paperTitle: "Analyzing and Improving the Image Quality of StyleGAN",
    venue: "CVPR",
    year: 2020,
    exactDate: "2020-09-08",
    citations: 8088,
    pdf: "https://arxiv.org/pdf/1912.04958",
    openSource: "https://github.com/NVlabs/stylegan2",
    type: "GAN",
    color: "#22c55e",
    primaryUseCase: "Image",
    bestPerformance: "FID 2.84 @ FFHQ 1024×1024",
    flowchart: "z → [mapping f] → w → [modulate+demodulate] → x\n",
    highLevelIntuition: "Random noise z is mapped to w, a disentangled latent vector.\nw modulates convolution weights for style control and artifact removal, then generates x.",
    pros: [
      "Removes characteristic \"droplet\" artifacts",
      "Improves FID and perceptual path length (PPL)",
      "Easier inversion/attribution of generated images"
    ],
    cons: [
      "Higher memory and compute for large models",
      "FID ↔ PPL trade-off on less-structured datasets",
      "No explicit shape/semantic controls"
    ],
    analogy: "A chef transforms rough, raw ingredients into a refined flavor blueprint, then expertly adjusts each cooking step—mixing, folding, seasoning—to smooth out imperfections and highlight crisp, clean tastes, crafting dishes that are both flawless and richly detailed.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "1912.04958",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DALL·E",
    model: "DALL·E",
    paperTitle: "Zero-Shot Text-to-Image Generation",
    venue: "arXiv",
    year: 2021,
    exactDate: "2021-01-05",
    citations: 6840,
    pdf: "https://arxiv.org/pdf/2102.12092",
    openSource: "https://github.com/openai/DALL-E",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "MS-COCO: Human eval, realism preference 90% (zero-shot)",
    flowchart: "flowcharts/gifs/DALL-E.gif",
    highLevelIntuition: "Text and image are both tokenized.\nTokens are concatenated and modeled autoregressively by a transformer.\nImage tokens are decoded back into pixels via dVAE.",
    pros: [
      "Zero-shot text-to-image generation",
      "Flexible compositional abilities",
      "Scales well with large datasets"
    ],
    cons: [
      "Limited fine-grained detail in outputs",
      "Worse performance on specialized distributions (e.g., CUB)",
      "High compute and data requirements"
    ],
    analogy: "A chef reads a detailed recipe written in code and translates it step-by-step into a dazzling dish—combining instructions and ingredient cues in perfect sequence—then uses a secret decoding technique to turn the final mix into a vivid, mouthwatering plate.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2102.12092",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DDIM",
    model: "Denoising Diffusion Implicit Models",
    paperTitle: "Denoising Diffusion Implicit Models",
    venue: "ICLR",
    year: 2021,
    exactDate: "2021-01-26",
    citations: 9843,
    pdf: "https://arxiv.org/pdf/2010.02502",
    openSource: "https://github.com/ermongroup/ddim",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 FID: 4.16 (100 steps, η=0.0)",
    flowchart: "flowcharts/gifs/DDIM.gif",
    highLevelIntuition: "Start from pure noise. At each step, predict and remove noise to denoise the sample. Repeat until you obtain a clean image.",
    pros: [
      "10×–50× faster sampling than DDPM",
      "High-quality samples with fewer steps",
      "Enables semantically meaningful latent interpolation"
    ],
    cons: [
      "Slight quality drop for extreme fast sampling",
      "Deterministic process may reduce sample diversity",
      "Minor detail loss in very short trajectories"
    ],
    analogy: "A chef begins with a bowl of fully spiced, chaotic stew and swiftly tastes and adjusts—removing excess seasoning step by step in a smooth, deliberate way—until the dish becomes clear, balanced, and ready to serve much faster than usual.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2010.02502",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "ImprovedDDPM",
    model: "Improved DDPM",
    paperTitle: "Improved Techniques for Training Score-Based Generative Models",
    venue: "NeurIPS",
    year: 2022,
    exactDate: "2021-03-15",
    citations: 1380,
    pdf: "https://arxiv.org/pdf/2006.09011",
    openSource: "https://github.com/vedantroy/improved-ddpm-pytorch",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "CelebA 64×64: FID 10.23",
    flowchart: "x₀ → [add noise σ₁] → x̃₁ → [score net, Langevin steps] → x₁\n   → [add noise σ₂] → x̃₂ → [score net, Langevin steps] → x₂\n      ...\n   → [σ_L, Langevin steps] → x_L → [denoise] → sample\n",
    highLevelIntuition: "Start from random noise, iteratively denoise through a series of decreasing noise levels.\nEach stage refines details as noise decreases, guided by the score network.",
    pros: [
      "Scales to high-resolution images (up to 256×256)",
      "Generates high-fidelity, diverse samples",
      "More stable training and better FID than previous score-based models"
    ],
    cons: [
      "High computational cost due to many noise scales and steps",
      "Sensitive to noise schedule selection",
      "Slight drop in Inception Score compared to some GANs"
    ],
    analogy: "A chef begins with a wildly over-seasoned stew, then carefully and repeatedly tastes and adjusts—gradually dialing back the intensity in stages—until each layer of flavor is perfectly balanced and the dish emerges rich and refined.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2006.09011",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "VQGAN",
    model: "VQ GAN",
    paperTitle: "Taming Transformers for High-Resolution Image Synthesis (VQGAN)",
    venue: "CVPR",
    year: 2021,
    exactDate: "2021-04-27",
    citations: 3750,
    pdf: "https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf",
    openSource: "https://github.com/CompVis/taming-transformers",
    type: "VAE + AR",
    color: "#eab308",
    primaryUseCase: "Image",
    bestPerformance: "ImageNet 256×256: FID = 11.2 (class-conditional, Table 4)",
    flowchart: "x → [Encoder E] → ẑ → [Quantize q] → z_q → [Decoder G] → x̂\n                                ↓\n                 [Transformer models indices s]\n",
    highLevelIntuition: "Compress image into codebook indices using CNN+quantization.\nModel sequence of indices globally with a Transformer.\nDecode sequence to high-resolution image using decoder.",
    pros: [
      "High-resolution image synthesis (megapixel range)",
      "Outperforms prior codebook-based models (e.g., VQVAE-2)",
      "Unifies conditional and unconditional synthesis with one model"
    ],
    cons: [
      "Sampling is slower than GANs",
      "Reconstruction fidelity depends on codebook quality",
      "Requires careful tuning of compression and codebook size"
    ],
    analogy: "A chef first chops a dish into standardized bite-sized cubes, then uses a master planner to arrange these pieces into a global, harmonious pattern before finally assembling and plating the detailed, high-resolution feast.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2104.01068",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "GuidedDiffusion",
    model: "Guided Diffusion",
    paperTitle: "Diffusion Models Beat GANs on Image Synthesis",
    venue: "NeurIPS",
    year: 2021,
    exactDate: "2021-10-16",
    citations: 9945,
    pdf: "https://arxiv.org/pdf/2105.05233",
    openSource: "https://github.com/openai/guided-diffusion",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "ImageNet 128×128 FID: 2.97",
    flowchart: "x_T → [predict εθ] → x_{T-1} → ... → x_1 → [classifier guidance] → x_0\n",
    highLevelIntuition: "Start from noise. Gradually remove noise using learned denoising plus optional classifier guidance, until a clean image emerges.",
    pros: [
      "Surpasses GANs on image synthesis FID",
      "Trade-off knob for fidelity vs diversity (classifier guidance)",
      "State-of-the-art with few sampling steps (25 steps)"
    ],
    cons: [
      "Sampling is slower than GANs",
      "Requires labeled data for classifier guidance",
      "No diversity-fidelity trade-off on unlabeled data"
    ],
    analogy: "A chef starts with a chaotic, over-seasoned stew and carefully pares back the flavors step by step—using both their refined palate and a trusted sommelier’s advice—to guide the dish toward a perfectly balanced and vibrant final taste.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2105.05233",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "GLIDE",
    model: "GLIDE",
    paperTitle: "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
    venue: "PMLR",
    year: 2021,
    exactDate: "2022-01-20",
    citations: 4306,
    pdf: "https://arxiv.org/pdf/2112.10741",
    openSource: "https://github.com/openai/glide-text2im",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "FID (MS-COCO 256×256, zero-shot): 12.24",
    flowchart: "x₀ → [add noise ε] → x_t → [predict ε̂_θ(x_t|c)] → x_{t-1}\n",
    highLevelIntuition: "Add random noise to the image step-by-step.\nThe model learns to predict and remove noise, guided by the text prompt, until a clear image forms.",
    pros: [
      "Generates highly photorealistic and diverse images",
      "Strong text prompt alignment (caption similarity)",
      "Supports text-guided image editing and inpainting"
    ],
    cons: [
      "Slow sampling (∼15s per image on A100 GPU)",
      "Sometimes struggles with highly unusual prompts",
      "Model/data can reflect or amplify dataset biases"
    ],
    analogy: "A chef starts by tossing a messy, noisy mix of ingredients into the pot, then steadily tastes and adjusts—using a detailed menu description as their guide—to remove unwanted flavors and shape the dish into a clear, photorealistic masterpiece.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2112.10741",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "VAEBM",
    model: "VAE-EBM",
    paperTitle: "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models",
    venue: "ICLR",
    year: 2021,
    exactDate: "2022-02-08",
    citations: 143,
    pdf: "https://arxiv.org/pdf/2010.00654.pdf",
    openSource: "https://github.com/NVlabs/VAEBM",
    type: "VAE + EBM",
    color: "#eab308",
    primaryUseCase: "Image",
    bestPerformance: "CIFAR-10 FID: 12.19",
    flowchart: "ε_z, ε_x → [VAE: T_θ] → (z, x₀) → [EBM: Langevin MCMC] → x_sample\n",
    highLevelIntuition: "Sample noise to generate a VAE image.\nUse MCMC and the energy function to refine this sample for better data alignment.",
    pros: [
      "Combines VAE's efficient sampling and latent space with EBM's sample refinement",
      "Outperforms state-of-the-art VAEs and EBMs on several benchmarks",
      "Provides strong out-of-distribution detection and full mode coverage"
    ],
    cons: [
      "Requires MCMC for sampling, which is slower than GANs",
      "MCMC chains can have poor long-run mixing",
      "Exact likelihood computation is intractable for high-dimensional data"
    ],
    analogy: "A chef first whips up a quick draft of a dish from a simple recipe, then patiently simmers and tweaks it through slow, careful tasting cycles—using intuition and refined judgment—to elevate the meal into a rich, perfectly balanced feast.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2010.00654",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "ProgDistil",
    model: "Progressive Distillation",
    paperTitle: "Progressive Distillation for Fast Sampling of Diffusion Models",
    venue: "ICLR",
    year: 2022,
    exactDate: "2022-02-24",
    citations: 1581,
    pdf: "https://arxiv.org/pdf/2202.00512",
    openSource: "https://github.com/google-research/google-research/tree/master/diffusion_distillation",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "FID 3.0 on CIFAR-10 in 4 steps",
    flowchart: "x₀ → [add noise] → z_t → [student predicts x̃] → z_{t-1/N} → ... → x̂\n",
    highLevelIntuition: "Noise is added to an image.\nStudent model predicts a denoised version to jump multiple teacher steps at once.",
    pros: [
      "Orders of magnitude faster sampling",
      "Maintains high perceptual quality with few steps",
      "Computationally efficient distillation procedure"
    ],
    cons: [
      "Some quality loss below 4 steps",
      "Student loses generality (works on discrete times)",
      "Distilled models less suited for stochastic sampling"
    ],
    analogy: "A chef trains an apprentice to skip slow, step-by-step stirring by mastering a bold technique that rapidly transforms a rough stew into a polished dish—cutting hours of simmering down to minutes while keeping the flavors rich and satisfying.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2202.00512",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "PaLM",
    model: "Pathways Language Model",
    paperTitle: "PaLM: Scaling Language Modeling with Pathways",
    venue: "JMLR",
    year: 2022,
    exactDate: "2022-03-15",
    citations: 7019,
    pdf: "https://arxiv.org/pdf/2204.02311",
    openSource: "https://github.com/conceptofmind/PaLM",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "Outperformed average human on BIG-bench (150 tasks, 5-shot)",
    flowchart: "x₀ → [input embedding] → h₁ → ... → h_L → [output embedding] → y₀\n",
    highLevelIntuition: "Each input token is embedded, passed through many Transformer layers, and transformed into logits.\nAt each step, the model predicts the next token based on prior context.",
    pros: [
      "State-of-the-art few-shot results on hundreds of language tasks",
      "Breakthrough performance on multi-step reasoning and multilingual tasks",
      "Highly efficient, scalable dense Transformer (540B params)"
    ],
    cons: [
      "High resource/training cost (6144 TPU v4 chips)",
      "Occasional instability/spikes during large-scale training",
      "Some bias/toxicity and memorization risks remain"
    ],
    analogy: "A master chef layers countless spices and techniques, passing ingredients through many stages of refinement—each step building on the last—to craft complex, nuanced dishes that impress across a vast banquet of diverse cuisines.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2204.02311",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "Imagen",
    model: "Imagen",
    paperTitle: "Imagen: Text-to-Image Diffusion Models",
    venue: "arXiv",
    year: 2022,
    exactDate: "2022-06-02",
    citations: 7283,
    pdf: "https://arxiv.org/pdf/2205.11487",
    openSource: "https://github.com/lucidrains/imagen-pytorch",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "COCO Zero-shot FID-30K: 7.27",
    flowchart: "Text → [frozen T5 encoder] → text embeddings\n  ↓\n[Diffusion model, classifier-free guidance]\n  ↓\nx₀ (noise) → xₜ → [predict ε̃ₜ] → xₜ₋₁ → ... → 64×64 image\n  ↓\n[Super-resolution diffusion]\n  ↓\n256×256 image → 1024×1024 image\n",
    highLevelIntuition: "Text is encoded for deep semantic understanding. Diffusion iteratively denoises from pure noise to image, guided by text. Super-resolution models upsample in stages for final high fidelity.",
    pros: [
      "Unprecedented photorealism and image-text alignment",
      "Uses large frozen language models for superior semantic understanding",
      "State-of-the-art performance without COCO training"
    ],
    cons: [
      "Limited ability to generate realistic people",
      "Social/cultural biases from web-scale data",
      "Not open-sourced due to ethical concerns"
    ],
    analogy: "A chef reads a richly detailed recipe, then gradually transforms a chaotic mix of raw ingredients into a stunning dish—layer by layer refining flavors and textures, finally plating an exquisite, high-resolution feast that perfectly matches the original description.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2205.11487",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "LDM",
    model: "Latent Diffusion Model",
    paperTitle: "High-Resolution Image Synthesis with Latent Diffusion Models",
    venue: "CVPR",
    year: 2022,
    exactDate: "2022-08-10",
    citations: 22057,
    pdf: "https://arxiv.org/pdf/2112.10752",
    openSource: "https://github.com/CompVis/latent-diffusion",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Image",
    bestPerformance: "FID 2.4 on ImageNet 256×256 super-resolution (Tab. 5, “LDM-4 (big, 100 steps)”)",
    flowchart: "x₀ → [Encoder E] → z₀ → [add noise] → z_t → [predict εθ(z_t, t)] → z_{t-1} ... → z₀ → [Decoder D] → x̂\n",
    highLevelIntuition: "Compress image to latent space.\nRun diffusion in latent space to denoise and generate.\nDecode latent to output high-res image.",
    pros: [
      "Significantly reduces training and inference costs vs. pixel-based DMs",
      "High visual fidelity; competitive or SOTA on inpainting, super-resolution, text-to-image",
      "Flexible cross-attention conditioning enables text, layout, and image guidance"
    ],
    cons: [
      "Sequential sampling still slower than GANs",
      "Minor quality loss if task needs precise pixel-level reconstruction",
      "Performance may degrade with excessive latent compression"
    ],
    analogy: "A chef first compresses a complex dish into a concentrated, flavorful paste, then gently refines and perfects this paste step by step, finally re-expanding it into a full, high-quality meal bursting with rich, detailed tastes.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2112.10752",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "LLaMA",
    model: "LLM Meta AI",
    paperTitle: "LLaMA: Open and Efficient Foundation Language Models",
    venue: "arXiv",
    year: 2023,
    exactDate: "2023-02-03",
    citations: 18366,
    pdf: "https://arxiv.org/pdf/2302.13971",
    openSource: "https://github.com/meta-llama/llama",
    type: "AR",
    color: "#ff6b00",
    primaryUseCase: "Text",
    bestPerformance: "LLaMA‑65B: 85.3% on BoolQ (zero‑shot, Table 3) / State‑of‑the‑art on TriviaQA (zero‑shot, Table 5)",
    flowchart: "x₀ → [Tokenize & Embed] → h₁ → [Transformer Layers] → h_L → [Softmax] → x̂₁\n",
    highLevelIntuition: "Text is tokenized and embedded. The transformer layers repeatedly refine this representation to predict the next token in sequence.",
    pros: [
      "Outperforms larger models (e.g., LLaMA‑13B > GPT‑3)",
      "Trained only on public data",
      "Efficient inference at smaller model sizes"
    ],
    cons: [
      "Still lags top models (Chinchilla, PaLM) on some benchmarks",
      "Slightly higher toxicity/bias with model size",
      "Limited books/academic data in pre-training"
    ],
    analogy: "A skilled chef chops ingredients finely and repeatedly refines their blend through multiple rounds of tasting and adjusting—layering flavors step by step—to craft dishes that rival those made by far larger kitchens.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2302.13971",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "SEDD",
    model: "Score Entropy Discrete Diffusion",
    paperTitle: "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
    venue: "ICML",
    year: 2023,
    exactDate: "2023-03-30",
    citations: 139,
    pdf: "https://arxiv.org/pdf/2310.16834",
    openSource: "https://github.com/louaaron/Score-Entropy-Discrete-Diffusion",
    type: "Diffusion",
    color: "#f43f5e",
    primaryUseCase: "Text",
    bestPerformance: "Beats GPT-2 on zero-shot language modeling perplexity (e.g. LAMBADA: SEDD ≤ 50.92 vs GPT-2 45.04; One Billion Words: SEDD Absorb ≤ 32.79 vs Autoregressive 31.98)",
    flowchart: "x₀ → [add noise Q_t] → x_t → [predict ratios sθ(x, t)] → x_{t-Δt} (reverse) → … → x₀\n",
    highLevelIntuition: "Add structured noise to data with a known rule.\nLearn to predict probability ratios that optimally denoise any noised input.",
    pros: [
      "Matches or beats GPT-2 on zero-shot perplexity tasks",
      "6–8× better generative perplexity than un-annealed GPT-2",
      "Flexible controllable generation (infilling, arbitrary prompting)"
    ],
    cons: [
      "Slightly larger parameter count due to time conditioning",
      "Inference may require many sampling steps",
      "Performance sensitive to noise schedule and loss weighting"
    ],
    analogy: "A chef intentionally adds a precise amount of seasoning noise to a dish, then learns to taste and adjust by estimating exactly how much flavor to remove at each step—undoing the seasoning in controlled stages to perfectly restore the original taste.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2310.16834",
    tags: [],
    authors: [],
    embedding: []
  },
  {
    id: "DiscreteVAE",
    model: "Discrete VAE",
    paperTitle: "An Introduction to Discrete Variational Autoencoders",
    venue: "arXiv",
    year: 2025,
    exactDate: "2023-08-07",
    citations: 0,
    pdf: "https://arxiv.org/pdf/2505.10344",
    openSource: "https://github.com/alanjeffares/discreteVAE",
    type: "VAE",
    color: "#eab308",
    primaryUseCase: "Image",
    bestPerformance: "MNIST test ELBO: –94.2",
    flowchart: "x → [encoder fϕ] → (D categorical params) → [sample z₁, ..., z_D] → [decoder gθ] → x̂\n",
    highLevelIntuition: "Encode input as D categorical codes.\nSample one-hot choices per latent.\nDecode sampled codes to reconstruct the input.",
    pros: [
      "Naturally handles discrete data modalities",
      "Avoids posterior collapse",
      "Enables interpretable, categorical latent codes"
    ],
    cons: [
      "Gradient estimation is high-variance",
      "Lacks reparameterization trick for discrete latents",
      "Scaling to large K or D is difficult"
    ],
    analogy: "A chef breaks a dish into distinct flavor categories—like sweet, sour, and spicy—then picks one dominant note from each category to build a new plate, combining these clear, one-of-a-kind tastes to recreate the original meal.",
    eli5AnalogyStyle: "vegetable",
    arxivId: "2505.10344",
    tags: [],
    authors: [],
    embedding: []
  }
];

export default models;
